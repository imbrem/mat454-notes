\documentclass{article}
\usepackage[margin=1in]{geometry}
\usepackage{mat454}

\usepackage[backend=biber]{biblatex}
\addbibresource{references.bib}

\usepackage{hyperref}
\hypersetup{
  colorlinks,
  linkcolor={red!50!black},
  citecolor={blue!50!black},
  urlcolor={blue!80!black}
}

\title{MAT454 Notes}
\author{Jad Elkhaleq Ghalayini}

\begin{document}

\maketitle

These notes are based off a combination of my class notes and handwritten notes generously provided by Professor Edward Bierstone. While I made my best efforts to ensure their contents are correct, there may be a variety of errors and inconsistencies, all of which are my own.

\tableofcontents
\newpage

\section{Review of Basic Complex Analysis}

\subsection{Elementary Properties of Holomorphic Functions}

The main objects of study in this course are holomorphic functions.
\begin{definition}[Holomorphic function]
\(f(t)\) is called \textbf{holomorphic at \(z \in \mbb{C}\)} if
\begin{equation}\lim_{h \to 0}\frac{f(z + h) - f(z)}{h}\end{equation}
exists, i.e. there is \(c \in \mbb{C}\) such that
\begin{equation}f(z + h) = f(z) + c \cdot h + \varphi(h) \cdot h, \lim_{h \to 0}\varphi(h) = 0\end{equation}
\end{definition}
Now, from this perspective, this looks no different from the usual case of a differentiable function. But it is different, because the variables are complex, and hence we can write
\begin{equation}z = x + iy, \qquad f(z) = u(z) + iv(z)\end{equation}
Hence, this function mapping \(z \mapsto f(z)\) is, from the real perspective, a function from
\(\reals^2 \to \reals^2\), taking
\begin{equation}\begin{pmatrix} x \\ y \end{pmatrix} \mapsto \begin{pmatrix} u(x, y) \\ v(x, y) \end{pmatrix}\end{equation}
Naturally, in the above definition, we can also write \(a + ib\) and \(h = \xi + i\eta\). Hence the derivative \(h \mapsto c \cdot h\) can be written as
\begin{equation}\begin{pmatrix} \xi \\ \eta \end{pmatrix} \mapsto
\begin{pmatrix} a & -b \\ b & a \end{pmatrix}
  \begin{pmatrix} \xi \\ \eta \end{pmatrix}
= \begin{pmatrix}\prt{f}{x} & \prt{f}{y}\end{pmatrix}
  \begin{pmatrix} \xi \\ \eta \end{pmatrix}\end{equation}
In other words, this says that
\begin{equation}\prt{f}{x} + i\prt{f}{y} = 0
\iff \prt{u}{x} = \prt{v}{y} \land \prt{u}{y} = -\prt{v}{x}\end{equation}
These are what is called the Cauchy-Riemann equations. So the moral of the story is that holomorphic is \textit{not} the same as differentiable as a function of two real variables. It' the same as differentiable as a function of two real variables \textit{plus} satisfying the Cauchy-Riemann equations.

It's going to convenient throughout this course to think about derivatives in terms of differential forms. Let's suppose, to begin a bit more generally, that we're considering a complex-valued \textit{differentiable} (not necessarily holomorphic) function \(f(x, y)\).
\begin{definition}
The \textbf{differential} of \(f\) is given by
\begin{equation}df = \prt{f}{x}dx + \prt{f}{x}dy\end{equation}
\end{definition}
But, we're thinking about \(x\) and \(y\) as parts of a complex number, with \(z = x + iy\) and \(\bar{z} = x - iy\). So we can solve for \(x\) and \(y\) in terms of \(z\) and \(\bar{z}\). We can also compute the differentials
\begin{equation}dz = dx + idy, \qquad d\bar{z} = dx - idy\end{equation}
So we can solve for \(dz\) and \(d\bar{z}\) in terms of \(dz\) and \(d\bar{z}\), getting
\begin{equation}dx = \frac{1}{2}(dz + d\bar{z}), \qquad dy = \frac{1}{2i}(dz - d\bar{z})\end{equation}
In particular, we can take \(df\) and rewrite it in terms of \(dz\) and \(d\bar{z}\) by substituting in these expressions. So if we do that we get
\begin{equation}df = \frac{1}{2}\left(\prt{f}{x} - i\prt{f}{y}\right)dz + \frac{1}{2}\left(\prt{f}{x} + i\prt{f}{y}\right)d\bar{z}\end{equation}
So, if we would like to define partial derivatives with respect to \(z\) and \(\bar{z}\), how should we define them? Well... the coefficients above seem to be natural choices, giving
\begin{equation}\prt{f}{z} = \frac{1}{2}\left(\prt{f}{x} - i\prt{f}{y}\right),
  \qquad \prt{f}{\bar{z}}
= \frac{1}{2}\left(\prt{f}{x} + i\prt{f}{y}\right)
\implies df = \prt{f}{z}dz + \prt{f}{\bar{z}}d\bar{z}\end{equation}
In terms of \textit{this} expression, what's a third way of writing the Cauchy-Riemann equations? It's simply
\begin{equation}\prt{f}{\bar{z}} = 0\end{equation}
And of course, this basically captures your ``feeling" of what a holomorphic function should be: it's supposed to be a function of \(z\), and not \(\bar{z}\). Ok, so this is the basic definition of holomorphic.

\subsection{Harmonic Functions}

We'll now say a few words about harmonic functions. Recall the following definition
\begin{definition}[Harmonic]
We say a real or complex valued function \(f(x, y)\) is \textbf{harmonic} if \(f\) is \(\mc{C}^2\) and
\begin{equation}\prt{^2f}{x^2} + \prt{^2f}{y^2} \iff \prt{^2f}{z\partial\bar{z}} = 0\end{equation}
\end{definition}
The above is known as \textbf{Laplace's equation}. It's immediate from the definition that a complex valued function is harmonic if and only if its real and imaginary parts are harmonic, and furthemore that every holomorphic function is harmonic. In particular, then, the real and imaginary parts of a holomorphic function are harmonic. On the other hand, maybe a slightly less immediate thing is that every real-valued harmonic function is, not necessarily everywhere but at least \textit{locally}, the real part of a holomorphic function. Why?

Well let's look at Laplace's equation. We know that Laplace's equation is satisfied, which tells us that
\begin{equation}\prt{}{\bar{z}}\left(\prt{g}{z}\right) = 0\end{equation}
So this of course tells us that \(\prt{g}{z}\) is holomorphic. And why does the result follow from this? Because every holomorphic function locally has a primitive which is holomorphic. Where does that come from? The fact that a closed form is locally exact, which is essentially saying it is a consequence of Cauchy's theorem. Another way of thinking about it, which is really also saying it is a consequence of Cauchy's theorem, is that \(\prt{g}{z}\) is given by a convergent power series and hence can be locally integrated into another convergent power series. So this is really ``one way or another by Cauchy's theorem".

The global result, on the other hand, does not necessarily follow, in brief, because we can ``loop around once". For example, \(\log|z|\) is a real-valued harmonic function in \(\mbb{C} \setminus \{0\}\), but it's not \textit{globally} the real part of a holomorphic function in \(\mbb{C} \setminus \{0\}\), because \(\log z\) has no single-valued branch here. This is a counterexample, but not on \(\mbb{C}\). Whether there are counterexamples in \(\mbb{C}\) is a very good question, and we'll deal with that when we get to Cauchy's theorem. It definitely is a topological question.

\subsection{The Riemann Sphere}

This is just a very brief recollection of the basic definitions of holomorphic and harmonic functions. I want to also recall, though maybe not all of you are familiar with this, the definitions of the various kinds of functions we're going to be working with as well as the spaces these functions are going to be defined on. In particular, everyone in a first-year course in complex variables has seen in some way the fact that its reasonable to say what you mean by ``holomorphic at \(\infty\)", and it can be useful to think about that. So, what \textit{do} you mean when you say that \(f(z)\) is holomorphic at \(\infty\)? Without introduing anything new, we can say that this means \(f(1/z)\) is holomorphic at \(0\). This is a very useful thing. We would like to make sense of this in a sort of well-structured way, and one does that by extending the complex plane to include the point at \(\infty\), or rather, to think of our functions not as on the complex plane, but on the extended complex plane including the point at \(\infty\), which is also called the Riemann sphere.

We have to say what the complex structure of that space is in a neighborhood of infinity, in such a way that captures this intuition, such that our holomorphic functions are holomorphic functions defined on open neighborhoods of the Riemann sphere.
Of course, complex-valued functions which are holomorphic on the \textit{whole} Riemann sphere are rather uninteresting, considering they are all constant by Liouville's principle. If we're allowed to consider holomorphic functions on the Riemann sphere \textit{with values on the Riemann sphere}, however, then we're back in interesting territory.

\begin{definition}[Stereographic Projection]
Consider the unit sphere \(S^2 = \{x^2 + y^2 + z^2 = 1\}\), and identify \(\reals^2\) with \(\mbb{C}\) by the isomorphism \((x, y) \mapsto z = x + iy\). Define the north pole \((0, 0, 1)\). We can define the \textbf{stereographic projection from the north pole} from \(\pi: S^2 \setminus N \to \mbb{C}\) to map a point \(s \in S \setminus N\) to the intersection of the line between \(s = (x, y, t)\) and \(N\) and the \(xy\) plane. Because the points \(s, N, (x/(1 - t), y/(1 - t), 0)\) must be colinear, we can define
\begin{equation}\pi(x, y, t) = \frac{x + iy}{1 - t}\end{equation}
This is a homeomorphism from \(S^2 \setminus N\) to \(\mbb{C}\).
\label{def:stereographic}
\end{definition}
A quick question: is this a \textit{metric} isomorphism? \textbf{No}: points very close together on the sphere can map to points very far from each other in the plane. This, however, is going to be a very important point in this course: we will study the behaviour of holomorphic functions according to the two natural metrics on the sphere: the induced metric on \(\reals^3\) i.e. the \textbf{chordal metric}, equivalent to the \textbf{geodesic metric}.

So the above homeomorphism gives a complex structure to the unit sphere minus the north pole. If we wanted to, we could get a complex structure on the unit sphere minus the south pole \(S = (0, 0, -1)\) by taking the stereographic projection from there. But we don't want to do that, because the complex structure we'd get would be incompatible. Instead, we want to take the \textit{complex conjugate} of a stereographic projection from the south pole,
\begin{equation}z' = \frac{x - iy}{1 + t}\end{equation}
So what was the point about compatibility? Well, what's the relationship between \(z\) and \(z'\)? We have
\begin{equation}z \cdot z' = \frac{x^2 + y^2}{(1 - t)^2} = 1 \implies z' = \frac{1}{z}\end{equation}
This is a holomorphic function from \(\mbb{C} \setminus \{0\} \to \mbb{C} \setminus \{0\}\) with a holomorphic inverse. So the two complex structures defined on the sphere minus the north pole and the sphere minus the south pole are compatible. By a \textit{complex structure} on a set, we mean a homeomorphism between an open subset that set and an open subset of the complex plane. If you're familiar with the language of manifolds, each of these two mappings is a \textit{coordinate chart}. These are even better than manifolds, though, because the coordinate charts are not just differentiable or infinitely differentiable, but holomorphic, or even better, \textit{rational}.

\subsection{Differential Forms}

We now proceed to some basic properties of differential forms, starting of course with the definition
\begin{definition}[Differential Form]
\TODO{this}
\end{definition}

\begin{theorem}
Let \(\Omega \subset \reals^2\), and let \(\omega = Pdx + Qdy\) be a differential form with \(P, Q\) continuous and real or complex-valued. Let \(\gamma = (x, y): [a, b] \to \Omega\) be a piecewise \(C^1\) curve in \(\Omega\). Then
\begin{equation}
  \int_\gamma\omega = \int_a^bf(t)dt,
  \quad f(t) = P(x(t), y(t))x'(t) + Q(x(t), y(t))y'(t) = \gamma^*\omega
\end{equation}
\end{theorem}

Note we can define \(\gamma^*\) recursively by
\begin{equation}
  \gamma^*(P) = P\circ \gamma,
  \quad \gamma^*(dx) = d(x \circ \gamma) = x'(t)dt,
  \quad \gamma^*(dy) = d(y \circ \gamma) = y'(t)dt
\end{equation}

\begin{theorem}[Independence of Parameter]
Let \(t: [c, d] \to [a, b]\) be an arbitrary differentiable function such that \(t(c) = a\), \(t(d) = b\) and \(t' > 0\). Define the curve \(\delta(s) = \gamma(t(s))\). Then for any differential form \(\omega\),
\[\int_\gamma\omega = \int_\delta\omega\]
\end{theorem}
\begin{proof}
Integration by substitution.
\end{proof}

\begin{definition}[Exterior Derivative, Primitive]
We define the \textbf{exterior derivative \(d\)} of a differentiable function \(F: \Omega \subset \reals^2 \to \mbb{C}\) to be a differential form
\begin{equation}
  \omega = dF = \prt{F}{x}dx + \prt{F}{y}dy
\end{equation}
In this case we call \(F\) a \textbf{primitive} of \(\omega\).
\end{definition}

We note that, for holomorphic \(F: \Omega \to \mbb{C}\) and piecewise differentiable \(\gamma: [a, b] \to \Omega\), the Fundamental Theorem of Calculus yields
\begin{equation}
  \int\gamma dF = \int_a^b(F \circ \gamma)'(t)dt = F(\gamma(b)) - F(\gamma(a))
\end{equation}
This observation implies the ``only if" in the following theorem:
\begin{theorem}
\(\omega\) has a primitive in \(\Omega\) if and only if, for any piecewise differentiable closed curve \(\gamma: [a, b] \to \Omega\) (i.e. with \(\gamma(a) = \gamma(b)\)), or equivalently any piecewise differentiable \(\gamma: S^1 \to \Omega\), we have
\begin{equation}
  \int_\gamma\omega = 0
\end{equation}
\end{theorem}
\begin{proof}
As stated above, this is a necessary condition as, if \(\omega\) has a primitive \(F\),
\begin{equation}
  \gamma(a) = \gamma(b) \implies F(\gamma(a)) = F(\gamma(b))
  \implies \int_\gamma\omega = \int_\gamma dF = F(\gamma(a)) - F(\gamma(b)) = 0
\end{equation}
This is a sufficient condition as
\TODO{rectangle argument}
\end{proof}

\begin{theorem}[Green's formula]
\begin{equation}\int_\gamma Pdx + Qdy = \iint_A\left(\prt{Q}{x} - \prt{P}{y}\right)dxdy\end{equation}
\end{theorem}

\TODO{text}

\begin{definition}[TODO]
\(\omega = Pdx + Qdy \implies d\omega = \left(\prt{Q}{x} - \prt{P}{y}\right)dx \wedge dy\)
\end{definition}

\TODO{rest}

\begin{definition}
We say a differential form \(\omega\) on a domain \(\Omega\) is \textbf{closed} if every point in \(\Omega\) has a neighborhood in which \(\omega\) has a primitive.
\end{definition}

We can show that \(\omega\) is closed if and only if \(\int_\gamma\omega = 0\) whenever \(\gamma\) is the boundary of a small enough rectangle in \(\Omega\).
Assuming \(\omega\) is \(\mc{C}^1\), then \(\omega\) is closed if and only if \(d\omega = 0\).

For example, consider \(\Omega = \mbb{C} \setminus \{0\}\) and the differential form \(\omega = \frac{dz}{z}\). This is closed in \(\Omega\), as a primitive exists locally, namely an appropriate branch of \(\log\), but \(\omega\) has no primitive, as letting \(\gamma(\theta) = e^{i\theta}\), we have
\begin{equation}
  \int_\gamma\frac{dz}{z} = \int_0^{2\pi}\frac{i\cancel{e^{i\theta}}}{\cancel{e^{i\theta}}}d\theta
  = 2i\pi \neq 0
\end{equation}

\begin{definition}
An open set \(\Omega\) is said to be \textbf{simply-connected} if every closed, continuous curve in \(\Omega\) is homotopic to a point in \(\Omega\).
\end{definition}

\begin{theorem}
Any closed differential form \(\omega\) in a simply-connected open set \(\Omega\) has a primitive.
\end{theorem}

A closed form \(\omega\) in a domain \(\Omega\) does not necessarily have a single-valued promitive, but has a \textbf{primitive along a curve}

\TODO{primitive along a curve}

\TODO{primitive over homotopy}

\begin{theorem}[Cauchy's Theorem]
Let \(\Omega\) be a domain and let \(f(z)\) be continuous in \(\Omega\) and holomorphic except on a set of discrete lines and points. Then the differentiable form \(f(z)dz\) is closed.
\end{theorem}
\begin{proof}
\TODO{this}
\end{proof}

\begin{corollary}
A holomorphic function \(f(z)\) locally has a proimitive, which is holomorphic (i.e. a function \(F\) such that \(dF = f(z)dz\))
\end{corollary}

\subsection{Cauchy's Integral Formula}

\begin{definition}
Let \(\gamma: S^1 \to \Omega\) be a closed curve and \(a \notin \gamma(S^1)\) be a point not in the image of \(\gamma\). Then the \textbf{winding number of \(\gamma\) with respect to \(a\)} is given by the integral
\begin{equation}
  w(\gamma, a) = \frac{1}{2\pi i}\int_\gamma\frac{dz}{z - a}
\end{equation}
This integral is an integer as
\TODO{why}
\end{definition}
As a function of \(a\), \(w(\gamma, a)\) is continuous on connected components of the complement of \(\gamma(S^1)\) (which we will just write as \(\gamma\) for convenence when confusion is unlikely).

\begin{theorem}[Cauchy's Integral Formula]
If \(f(z)\) is holomorphic in \(\Omega\), \(a \in \Omega\) and \(\gamma: S^1 \to \Omega\) is a closed curve with \(a \notin \gamma\), then
\begin{equation}
  \frac{1}{2\pi i}\int_\gamma\frac{f(z)dz}{z - a} = w(\gamma, a)f(a)
\end{equation}
\end{theorem}
\begin{proof}
\TODO{this}
\end{proof}

In summary, we have that, for \(f\) continuous on a domain \(\Omega\), the following conditions are equivalent
\begin{enumerate}

  \item \(f\) is holomorphic on \(\Omega\)

  \item \(f(z)dz\) is closed

  \item \(f(z) = \frac{1}{2\pi i}\int_\gamma\frac{f(\zeta)}{\zeta - a}d\zeta\) for \(z\) in an open disc with boundary \(\gamma\).

\end{enumerate}
e.g. if \(f\) is continuous and holomorphic except on a line, it is holomorphic.

\subsection{Integral Formulas for Taylor Coefficients}

\subsection{Cauchy's Inequalities}

Let's, as we usually do, consider a holomorphic function \(f(z)\) in an open set \(\Omega \subseteq \mbb{C}\). Last time, we showed that \(f\) has a convergent power series expansion in any open disc in \(\Omega\) (centered at the center of the disc). For example, around \(a = 0 \in \Omega\), we can write
\begin{equation}f(z) = \sum_{n = 0}^\infty a_nz^n\end{equation}
Writing \(z = re^{i\theta}\), we get
\begin{equation}f(re^{i\theta}) = \sum_{n = 0}^\infty a_nr^ne^{in\theta}\end{equation}
We can write out the following formula for these Fourier coefficients:
\begin{equation}a_nr^n = \frac{1}{2\pi}\int_0^{2\pi}e^{-in\theta}f(re^{i\theta})d\theta\end{equation}
Today we're going to be looking at the consequences of this formula. First of all, this formula gives a simple but useful upper bound on \(a_n\): if we take the maximum absolute value of \(f\) along the circle of radius \(r\), written
\begin{equation}M(r) = \sup_\theta|f(re^{i\theta})|\end{equation}
we get
\begin{equation}|a_n| \leq \frac{M(r)}{r^n}\end{equation}
These are called \textbf{Cauchy's inequalities}. These have some important consequences, like \textbf{Liouville's theorem}: a bounded holomorphic function on all of \(\mbb{C}\) is a constant. How does this follow? Well, if \(c\) is the upper bound of \(f\) on \(\mbb{C}\), we have each
\begin{equation}\forall r \in \reals^+, M(r) \leq c \implies |a_n| \leq \frac{M(r)}{r^n} \leq \frac{c}{r^n}\end{equation}
Hence, for \(n > 0\), \(0 \leq a_n \leq \epsilon\) for all \(\epsilon > 0\), implying \(a_n = 0\). It follows that \(f = a_0 = c\), a constant. Another consequence is that we can write, for any \(r\)
\begin{equation}f(0) = a_0 = a_0r^0 = \frac{1}{2\pi}\int_0^{2\pi}f(re^{i\theta})d\theta\end{equation}
This generalizes readily to stating that holomorphic functions satisfy the \textbf{Mean Value Property (MVP)}:
\begin{equation}f(\text{center of disk}) = \ \text{mean value on boundary}\end{equation}
Another property which we won't prove is the \textbf{Maximum Modulus Principle (MMP)}: if \(f\) is a continuous complex-valued function on an open \(\Omega \subseteq \mbb{C}\) with the MVP, then it satisfies the MMP, that is, if \(|f|\) has a local maximum at a point \(a\) of \(\Omega\), then \(f\) is constant in a neighborhood of \(a\).

We can use this to prove \textbf{Schwarz's Lemma}:
\begin{theorem}[Schwarz's Lemma]
Suppose \(f(z)\) is holomorphic in \(|z| < 1\), \(f(0) = 0\) and \(|f(z)| < 1\). Then
\begin{enumerate}

  \item \(|f(z)| \leq |z|\) if \(|z| < 1\)

  \item If \(|f(z_0)| = |z_0|\) at some \(z_0 \neq 0\), then \(f(z) = \lambda z\) for some \(|\lambda| = 1\).

\end{enumerate}
\end{theorem}
We recall a sketch of the proof
\begin{proof} (Sketch)
By the convergent power series expansion, \(g(z) / z\) is holomorphic, and can hence have the maximum modulus principle applied to it.
\end{proof}
So let's spend a little time looking at functions with the MVP in general. Continuous functions with the MVP are precisely the \underline{harmonic functions}. The real and imaginary parts of a complex valued function with the MVP also satisfy the MVP. A real valued harmonic function \(g\) is locally the real part of a holomorphic function, uniquely determined up to addition of a constant:
\begin{equation}
\prt{^2g}{z\partial\bar{z}} = 0 \implies \prt{g}{z} \ \text{holomorphic}
\end{equation}
Therefore, \(\prt{g}{z}\) locally has primitive \(f\), defined up to a constant. Since \(g\) is real valued, we can write
\begin{equation}df = \prt{g}{z}dz, \quad d\bar{f} = \prt{g}{\bar{z}}d\bar{z}\end{equation}
Hence,
\begin{equation}d(f + \bar{f}) = dg \implies g = 2\Re f + \ \text{const}\end{equation}
So harmonic functions satisfy the MVP and MMP, and conversely, a continuous function in an open set \(\Omega \subseteq \mbb{C}\) satisfying the MVP is harmonic. Just a couple of words as to why this is true, as this is really something that you should review: this comes from the solution to what's called the Dirichlet problem for a disk. What is this problem? It says that, given any continuous function \(f\) on the boundary of a disk \(|z| < r\), you can extend it to a continous function \(F\) on the whole disk which is harmonic on the interior.

\subsection{Zeros and Poles}

\begin{definition}[Zero]
If \(f\) is holomorphic in a neighborhood of \(z_0 \in \mbb{C}\) and \(f(z_0) = 0\), we can write, for some \(k \in \mbb{N}\),
\begin{equation}f(z) = (z - z_0)^kf_1(z)\end{equation}
where\(f_1(z)\) is nonvanishing near \(z_0\). In this case \(k\) is called the \textbf{order} or \textbf{multiplicity} of the \textbf{zero} \(z_0\)
\end{definition}
Zeros of holomorphic functions form a discrete set. We want to study, however, not only holomorphic functions, but also quotients of holomorphic functions
\begin{definition}[Meromorphic]
A function \(f\) is \textbf{meromorphic} on an open \(\Omega \subseteq \mbb{C}\) if it is defined and holomorphic in the complement of a discrete set such that in some neighborhood of every point of \(\Omega\) we can write
\(f(z) = g(z)/h(z)\)
where \(g, h\) are holomorphic and \(h\) is not identically zero.
\end{definition}
Why is it interesting to work with meromorphic and not just holomorphic functions? Essentially, it's because meromorphic functions in a domain \(\Omega\) form a field (whereas holomorphic functions only form a ring). Note that, in this course, when we say ``domain", what we mean is a connected open set.
If \(f(z), g(z)\) are holomorphic near \(z_0\), like before, we can write
\begin{equation}f(z) = (z - z_0)^kf_1(z), \qquad g(z) = (z - z_0)^\ell g_1(z)\end{equation}
where \(f_1(z_0), g_1(z_0) \neq 0\). Near \(z_0\), then, the quotient looks like
\begin{equation}\frac{f(z)}{g(z)} = (z - z_0)^{k - \ell}\frac{f_1(z)}{g_1(z)}\end{equation}
So what are the different possibilities? If \(k \geq \ell\), then this function extends to be holomorphic at \(z_0\). On the other hand, if \(k < \ell\), then, of course,
\begin{equation}\lim_{z \to z_0}\left|\frac{f(z)}{g(z)}\right| = \infty\end{equation}
Note: \textit{not} undefined, but \(\infty\). In this case, we say that \(z_0\) is a pole of order \(\ell - k\).
Holomorphic functions in an annulus \(r < |z| < R\) have a convergent Laurent expansion in an annulus
\begin{equation}\sum_{n = -\infty}^\infty a_nz^n = \sum_{n < 0}a_nz^n + \sum_{n \geq 0}a_nz^n\end{equation}
Note that the LHS converges when \(r < |z|\), whereas the RHS converges when \(|z| < R\). This actually comes from Cauchy's theorem, just in the case of a convergent power series expansion of a holomorphic function. So this is from Cauchy's integral formula:
\begin{equation}f(z) = \frac{1}{2\pi i}\int_{\gamma_1}\frac{f(\xi)}{\xi - z}d\xi - \frac{1}{2\pi i}\int_{\gamma_2}\frac{f(\xi)}{\xi - z}d\xi = \sum_{n = -\infty}^\infty a_nz^n\end{equation}
for \(z\) between \(\gamma_1, \gamma_2\).
So
\begin{equation}a_n = \frac{1}{2\pi i}\int_{\gamma_?}\frac{f(\xi)}{\xi^{n + 1}}d\xi\end{equation}
So of course, this should be just like before for the positive part. Note that this is the integral over \(\gamma_1\) if \(n \geq 0\) and over \(\gamma_2\) if \(n < 0\).

Previously, we talked about holomorphic functions on the Riemann sphere, noting there were very few of them: namely, constants. Now, there are a few more meromorphic functions on the Riemann sphere, but not much. Specifically,
\begin{theorem}
Every meromorphic function \(f\) on \(S^2\) is rational.
\end{theorem}
\begin{proof}
This theorem uses what is probably the only thing in first year calculus you don't prove: the partial fraction decomposition. So we prove it now.
Say \(f(z)\) has poles \(b_1,...,b_k\) (finite) and maybe \(\infty\). So what can we say about the Laurent expansion at a pole? There's only finitely many negative terms, specifically, the order of the pole.

So these negative parts of the Laurent expansions around each \(b_j\) are like polynomials \(P_j(\frac{1}{z - b_j})\). We'll call these principal parts. What about the principal part at \(\infty\)? It's a polynomial in \(z\), as it's a polynomial in \(\frac{1}{z'}\), where \(z'\) is the coordinate at \(\infty\), which is \(1/z\). Call this \(P_\infty(z)\). So we can write
\begin{equation}f(z) - P_\infty(z) - \sum_{j = 1}^kP_j\left(\frac{1}{z - b_j}\right)\end{equation}
which is holomorphic on \(S^2\), and hence must be a constant \(a\). So we can write
\begin{equation}f(z) = a + P_\infty(z) + \sum_{j = 1}^kP_j\left(\frac{1}{z - b_j}\right)\end{equation}
And that's rational.
\end{proof}
\begin{definition}[Isolated singularity]
A holomorphic function in a \underline{punctured} disk \(0 < |z| < R\) has an \textbf{isolated singularity} at \(0\) if \(f(z)\) cannot be extended to be holomorphic at \(0\).
\end{definition}
Extension is possible if and only if \(f\) is bounded in a neighborhood of \(0\) in this punctured disk.

\TODO{January 20}

\TODO{January 23}

\TODO{January 25}

\TODO{January 27}

\TODO{January 30}

\TODO{Feb 1}

\TODO{Feb 4}

\subsection{Residue Calculus}

\section{Topology of the Space of Holomorphic Functions}

\begin{definition}
Let \(\Omega \subset \mbb{C}\) be open. We define \(\mbb{C}(\Omega)\) to be the \textbf{ring of continuous, complex-valued functions on \(\Omega\)} and \(\mc{H}(\Omega)\) to be the \textbf{subring of holomorphic functions on \(\Omega\)}
\end{definition}
We want to assign a topology to \(\mc{C}(\Omega)\). We begin by defining some primitive notions:
\begin{definition}[Uniform convergence on compact subsets]
We say that a sequence of functions \(\{f_n\} \subset \mc{C}(\Omega)\) \textbf{converges uniformly on compact subsets} if for all compact subsets \(K \subset \Omega\), \(\{f_n | K\}\) converges uniformly, i.e.
\begin{equation}\forall \ \text{compact} \ K \subset \Omega, \forall \epsilon > 0, \exists N \in \mbb{N}, \forall m, n \geq N, \forall z \in K, |f_m(x) - f_n(x)| < \epsilon\end{equation}
\end{definition}
Uniform convergence on compact subsets can be used to determine a topology on \(\mc{C}(\Omega)\) as follows:
\begin{definition}[The Compact-Open Topology]
We define the \textbf{fundamental system of open neighborhoods of 0} as
\begin{equation}\forall \ \text{compact} \ K \subset \Omega, \forall \epsilon > 0, V(K, \epsilon) = \{f: \forall z \in K, |f(z)| < \epsilon\}\end{equation}
We note that the sequence \(\{f_n\}\) converges to \(f\) uniformly on compact subsets if and only if
\begin{equation}\forall K, \epsilon, \exists N \in \mbb{N}, \forall n \geq N, f - f_n \in V(K, \epsilon)\end{equation}
In a similar vein, open neighborhoods of any \(f \in \mbb{C}(\Omega)\) can be defined by translating neighborhoods of 0 by \(f\), determining a basis for a topology in which convergence is equivalent to uniform convergence on compact subsets, i.e. we define the basis of the \textbf{compact-open topology} to be
\begin{equation}\mc{B} = \bigcup_{f, K, \epsilon}\{f + V(K, \epsilon)\}\end{equation}
\end{definition}
In this topology, \(\mc{C}(\Omega)\) is metrizable, and in fact, this topology can alternatively be defined by a translation-invariant metric as follows:
\begin{definition}[Exhausion]
An infinite sequence of compact subsets \(K_1 \subset K_2 \subset ...\) such that \(\Omega = \bigcup_i K_i\) is called an \textbf{exhaustion of \(\Omega\)}
\end{definition}
\begin{definition}[Translation-invariant metric]
Given an exhaustion \(K_i\) of \(\Omega\), we define a metric
\begin{equation}d(f) = \sum_{i = 1}^\infty\frac{1}{2^i}\min\{1, M_i(f)\}, \quad M_i(f) = \max_{K_i}f\end{equation}
\end{definition}
This is a metric as
\begin{equation}d(f) = 0 \iff f = 0, \quad d(f + g) = d(f) + d(g) \leq d(f) + d(g)\end{equation}
We have that \(\mc{C}(\Omega)\) is \textbf{complete}, i.e. the limit of a sequence of functions that converges uniformly on compact sets is continuous. We given \(\mc{H}(\Omega)\) the subspace topology.
\begin{theorem}[Weierstrass]
\begin{enumerate}
  \item \(\mc{H}(\Omega)\) is a closed subspace of \(\mc{C}(\Omega)\), i.e. if \(\{f_n\} \subset \mc{H}(\Omega)\) converges uniformly to \(f\) on compact sets then \(f = \lim_{n \to \infty}f_n \in \mc{H}(\Omega)\) is holomorphic.
  \item The mapping \(\mc{H}(\Omega) \to \mc{H}(\Omega)\) \(f \mapsto f'\) is continous, i.e. if \(\{f_n\} \subset \mc{H}(\Omega)\) converges uniformly to \(f\) on compact sets then \(\{f_n'\}\) converges uniformly to \(f'\) on compact sets.
\end{enumerate}
\end{theorem}
\begin{proof}
\begin{enumerate}

  \item \TODO{this}

  \item \TODO{this}

\end{enumerate}
\end{proof}

We now consider an application of this result to series of holomorphic functions:
\begin{corollary}
Let \(\{f_n\}\) be a series of holomorphic functions. If \(\{g_n = \sum_{k = 0}^nf_k\}\) converges uniformly on compact subsets of \(\Omega\), then the sum
\begin{equation}f = \sum f_n\end{equation}
is holomorphic on \(\Omega\) and the series can be differentiated term by term.
\end{corollary}
\begin{proposition}
Let \(\Omega\) be a domain. If \(\{f_n\} \subset \mc{H}(\Omega)\) converges uniformly on compact sets and each \(f_n\) vanishes nowhere in \(\Omega\) then \(f = \lim_{n \to \infty}f_n\) is either never zero or identically zero.
\end{proposition}
\begin{proof}
\TODO{this}
\end{proof}
\begin{corollary}
Let \(\Omega\) be a domain. If \(\{f_n\} \subset \mc{H}(\Omega)\) converges uniformly on compact sets and each \(f_n\) is one-to-one, then \(\lim_{n \to \infty}f_n\) is either one-to-one or constant.
\end{corollary}
\begin{proof}
\TODO{this}
\end{proof}

\TODO{rest}

\subsection{Series of Meromorphic Functions}

Let \(\{f_n\}\) denote a sequence of meromorphic functions on an open set \(\Omega \subset \mbb{C}\).
\begin{definition}
We say that \(\sum_{n = 1}^\infty f_n\) \textbf{converges uniformly} (respectively \textbf{converges uniformly absolutely}) on \(X \subset \Omega\) if all but finitely many \(f_n\) have no pole in \(X\) and form a uniformly convergent (respectively uniformly absolutely convergent) series on \(X\).
\end{definition}
We consider series of meromorphic functions that converge uniformly on compact subsets of \(\Omega\). We can define the sum on relatively compact open \(U \subset \Omega\) as
\begin{equation}\sum_{n \leq n_0}f_n + \sum_{n > n_0}f_n\end{equation}
where \(f_n\) is meromorphic for \(n \leq n_0\) and \(f_n\) has no pole on \(U\) for \(n > n_0\) with the sum uniformly convergent on \(U\).
\begin{theorem}
Let \(\sum f_n\) be a series of meromorphic functions on \(\Omega\). If the series is uniformly convergent on compact subsets of \(\Omega\), then the sum is a meromorphic function \(f\) on \(\Omega\) and \(\sum f_n'\) converges uniformly on compact sets of \(\Omega\) to \(f'\).
\end{theorem}
We remark that the pole sets obey the inclusion \(P(f) \subset \bigcup_nP(f_n)\), with equality if the \(P(f_n)\) are pointwise disjoint, in which case if \(p\) is a pole of degree \(k\) for any \(f_n\) then it is a pole of degree \(k\) for \(f\).

\TODO{examples}

\subsection{The Weierstrass \(\wp\)-function}

\begin{definition}
Let \(e_1, e_2 \in \mbb{C}\) be linearly independent over \(\mbb{R}\). We can define a discrete subgroup of \(\mbb{C}\) with \textbf{basis} \(e_1, e_2\)
\begin{equation}\Gamma = \{n_1e_1 + n_2e_2 : n_1, n_2 \in \mbb{Z}\}\end{equation}
We say that \(f\) has \(\Gamma\) as \textbf{group of periods} if
\begin{equation}\forall z \in \mbb{C}, f(z) = f(z + e_1) = f(z + e_2)\end{equation}
\end{definition}
\begin{definition}[Weierstrass \(\wp\)-function]
We define the Weierstrass \(\wp\)-function by the infinite sum
\begin{equation}\wp(z) = \frac{1}{z^2} + \sum_{\substack{w \in \Gamma \\ w \neq 0}}\left[
  \frac{1}{(z - w)^2} - \frac{1}{w^2}
\right]\end{equation}
\end{definition}
We'll see that \(\wp\) converges uniformly and absolutely on compact subsets of \(\mbb{C}\).
\begin{lemma}
\(\sum_{\substack{w \in \Gamma \\ w \neq 0}}\frac{1}{|w|^2}\) converges
\end{lemma}
\begin{proof}
\TODO{this}
\end{proof}
\begin{claim}
\(\wp\) converges uniformly absolutely in any disc \(|z| \leq r\)
\end{claim}
\begin{proof}
\TODO{this}
\end{proof}
\begin{claim}
\begin{enumerate}
  \item \(\wp\) has a double pole at each \(w \in \Gamma\) with prime part \(\frac{1}{(z - w)^2}\)
  \item \(\wp\) is an even function
  \item \(\wp' = -z\sum_{w \in \Gamma}(z - w)^{-2}\) converges absolutely uniformly on compact subsets of \(\mbb{C}\)
  \item \(\wp'\) is doubly periodic: \(\forall w \in \Gamma, \wp'(z + w) = \wp'(z)\)
  \item \(\wp'\) is odd
  \item \(\wp\) itself has \(\Gamma\) as group of periods
\end{enumerate}
\end{claim}
\begin{proof}
\TODO{this}
\end{proof}
In summary, \(\wp\) is a meromorphic function with \(\Gamma\) as group of periods, and poles precisely at points of \(\Gamma\) each with prime part \(\frac{1}{(z - w)^2}\).

In some neighborhood of 0, we can write, because \(\wp\) is even,
\begin{equation}\wp(z) = \frac{1}{z^2} + a_2z^2 + a_4z^4 + ...\end{equation}
It follows that \(g(z) = \wp(z) - \frac{1}{z^2}\) vanishes at 0. We can find these coefficients by differentiating term-by term:
\begin{equation}a_2 = 3\sum_{w \neq 0}\frac{1}{w^4}, \quad a_4 = 5\sum_{w \neq 0}\frac{1}{w^6}, \quad ... \quad a_k = (2k + 1)\sum_{w \neq 0}\frac{1}{w^{2(k = 1)}}\end{equation}

\subsection{Differential equations solved by \(\wp\)}

Differentiating \(\wp\) term-by-term, we obtain
\begin{equation}\wp'(z) = -\frac{2}{z^3} + 2a_2z + 4a_4z^3 + ...\end{equation}
Squaring both sides, we obtain
\begin{equation}\wp'(z)^2 = \frac{4}{z^6} + \frac{8a_2}{z^2} - 16a_4 + ...\end{equation}
We note, however, that
\begin{equation}\wp(z)^3 = \frac{1}{z^6} + \frac{3a_2}{z^2} + 3a_4 + ...\end{equation}
Therefore, we compute that
\begin{equation}\wp'(z)^2 - 4\wp(z)^3 = -\frac{20a_2}{z_2} - 28a_4 + z^2(...)\end{equation}
Hence,
\begin{equation}(\wp')^2 - 4\wp^3 + 20a_2\wp + 28a_4\end{equation}
is holomorphic near zero, and zero at zero. Since it has \(\Gamma\)as group of periods, it is holomorphic near each \(w \in \Gamma\), and \(0\) at \(w \in \Gamma\). In fact, it is holomorphic in \(\mbb{C}\) as it has no poles outside \(\Gamma\), and therefore bounded in \(\mbb{C}\) by periodicity. It folows by Liouville's theorem that it is identically zero, and hence that
\begin{equation}\wp'^2 = 4\wp^3 - 20a_2\wp - 28a_4\end{equation}
Letting \(x = \wp(z), y = \wp'(z)\), gives a parametric representation of the algebraic curve
\begin{equation}y^2 = 4x_3 - 20a_2x - 28a_4\end{equation}
We'll show that any point \((x, y)\) of this curve is the image of a point \(z \in \mbb{C}\) which is uniquely determined up to addition by an element of \(\Gamma\).

\subsection{Doubly Periodic Functions}

Assume \(\Gamma\) is the group of periods generated by \(e_1, e_2\), which are linearly independent over \(\reals\).
\begin{proposition}
If \(f\) is a non-constant meromorphic function on \(\mbb{C}\) with \(\Gamma\) as group of periods, then the number of zeros of \(f\) in a period parallelogram is equal to the number of poles in the same parallelogram
\end{proposition}
\begin{proof}
\TODO{this}
\end{proof}
\begin{corollary}
A holomorphic function in \(\mbb{C}\) with \(\Gamma\) as group of periods is constant.
\end{corollary}
\begin{proof}
Otherwise, the number of zeros of \(f(z) - a\) is equal to the number of poles, which is zero for all \(a\), which is impossible. Note that we've already shown this previously using Liouville's theorem: this is just an alternative proof.
\end{proof}
\begin{proposition}
Let \(f\) be a nonconstant meromorphic function in \(\mbb{C}\) with \(\Gamma\) as group of periods, and let \(a \in \mbb{C}\). Let \(\alpha_k\) be the roots of \(f(z) = a\) (counted with multiplicity) and \(\beta_k\) be the poles of \(f(z)\) (again counted with multiplicity) contained in a period parallelogram. Then
\begin{equation}\sum\alpha_k = \sum\beta_k\end{equation}
(in particular, \(\sum\alpha_j \mod \Gamma\) is independent of \(\alpha\))
\end{proposition}
\begin{proof}
\TODO{this}
\end{proof}
\begin{theorem}
Given a discrete group \(\Gamma\), the equation \(4x^3 - 20a_2x - 28a_4 = 0\) where
\begin{equation}a_2 = 3\sum_{w \neq 0}\frac{1}{w^4}, \quad a_4 = 5\sum_{w \neq 0}\frac{1}{w^6}\end{equation}
has three distinct roots. Moreover, \(\forall (x, y) \in \mbb{C}^2\) on the algebraic curve
\begin{equation}y^2 = 4x^3 - 20a_2x - 28a_4\end{equation}
there is unique \(z \in \mbb{C}\) modulo \(\Gamma\) such that \(x = \wp(z)\), \(y = \wp'(z)\).

Conversely, given equation \(y^2 = 4x^3 - 20a_2x - 28a_4\) where the RHS has 3 distinct roots (any nonzero cubic!), there exists \(\Gamma\) such that \(a_2, a_4\) are given as above (and so if \(\wp\) is the Weierstrass \(\wp\)-function associated to \(\Gamma\), then \(x = \wp(z)\), \(y = \wp'(z)\) gives a parametrization of the curve as above).
\end{theorem}
\begin{proof}
\TODO{this}
\end{proof}

\subsection{Complex Projective Space}

\begin{definition}[\(n\)-dimensional complex projective space]
We define
\begin{equation}
  \CP{n} = \mbb{C}^{n + 1} \setminus \{0\} / \sim
\end{equation}
where
\begin{equation}
  (x_0,...,x_n) \sim (x_0',...,x_n') \iff \exists \lambda \in \mbb{C}, (x_0',...,x_n') = (\lambda x_0,..., \lambda x_n)
\end{equation}
We denote the equivalence class of \((x_0,...,x_n)\) by \([x_0,...,x_n]\).
\end{definition}
\begin{definition}[Homogeneous coordinates]
We define coordinate charts \(U_i = \{[x_0, ..., x_n] \in \CP{n} : x_i \neq 0\}\) with affine coordinates \(U_i \to \mbb{C}^n\),
\begin{equation}[x_0,...,x_n] \mapsto \left(\frac{x_0}{x_i},...,\frac{x_{i - 1}}{x_i}, \frac{x_{i + 1}}{x_i},...,\frac{x_n}{x_i}\right)\end{equation}
with inverse
\begin{equation}(g_1,...,g_n) \mapsto [g_1,...,g_{i - 1}, 1, g_{i + 1},..,g_n]\end{equation}
\end{definition}
Using these coordinates, we have that \(\CP{n}\) has the structure of an \(n\)-dimensional complex manifold, as the transition mappings are rational. Let's take one of the charts here, say \(U_0\), to be \(\mbb{C}^n\). So
\begin{equation}\CP{n} = U_0 \cup \ \text{everything else}\end{equation}
But what's everything else? So \(U_0\) is all the points where \(x_0 \neq 0\), so everything else is the set of points
\begin{equation}
  \{x_0 = 0\} = \{[0, x_1,...,x_n]\} \simeq \CP{n - 1}
  \implies \CP{n} = U_0 \cup \CP{n - 1}
\end{equation}
We call this copy of \(\CP{n - 1} \simeq \{x_0 = 0\}\) the \textbf{hyperplane at infinity}. This is like a generation of the Riemann sphere which we saw before, which we saw was given by \(S^2 = \CP{1}\). So when we talk about \(\CP{2}\), that's like having 2-complex coordinates with a line at infinity. Specifically, we can write it as
\begin{equation}
  \CP{2} = \{[x, y, t]\} = \mbb{C}^2_{(x, y)} \cup \{t = 0\}
\end{equation}
the \textbf{projective line at infinity}.
Now assume we have a curve \(X \subset \mbb{C}^2\) generated by the equation
\begin{equation}y^2 = 4x^3 - 20a_2x - 28a_4\end{equation}
where the RHS has three distinct roots. We want to compute the \textbf{compactification of \(X\) in \(\CP{2}\)}. We can write this down in homogeneous coordinates
\begin{equation}y^2t = 4x^3 - 20a^2xt^2 - 28a_4t^3\end{equation}
taking \(X'\) to be the solution set of this.
Why is this the right thing? When you look at \(\CP{2}\), and look in here at the set of points
\begin{equation}\{[x, y, t]: t \neq 0\} \simeq \mbb{C}^2_{(x, y)}\end{equation}
we see that it is has homomorphism
\begin{equation}[x, y, t] \mapsto \left(\frac{x}{t}, \frac{y}{t}\right)\end{equation}
Hence, we rewrite our eqaution in our new coordinates for \(\mbb{C}^2\),
\begin{equation}\frac{y^2}{t^2} = 4\frac{x^3}{t^3} - 20a_2\frac{x}{t} - 28a_4\end{equation}
Now we can just multiply both sides by \(t^3\). So if you haven't seen this before, this takes a little bit of familiarity, but the actual operations involved are very simple operations. Of course, our \textit{original} \(X\) is a subspace of \(X'\). But how much have we added to \(X\)? Well, if we set \(t = 0\), we get \(x = 0\). So, how many points are we adding? One point, at \(\infty\):
\begin{equation}X' = X \cup \{[0, 1, 0]\}\end{equation}
Now, in the neighborhood of any finite point, \(X'\) just looks like \(X\). What about in a neighborhood of the point at \(\infty\), \([0, 1, 0]\)? What does it look like?
So this point \([0, 1, 0]\) doesn't actually lie in the coordinate chart \(U_0 = \{x \neq 0\}\), it lies in \(U_1 = \{y \neq 0\}\). This chart has affine coordinates given by \((x', t') = (x/y, t/y)\). So what's the equation of \(X'\) in \textit{this} coordinate chart? It's
\begin{equation}t' = 4x'^3 - 20a_2x't'^2 - 28a_4t'^3\end{equation}
In some neighborhood of \((x', t') = (0, 0)\) (the point at infinity), the implicit function theorem tells us that \(t'\) is a holomorphic function of \(x'\):
\begin{equation}t' = 4x'^3 - 320a_2x'^7 + ...\end{equation}
As an extercise, we can take the Taylor series
\begin{equation}t' = b_0 + b_1x + b_2x^2 + ...\end{equation}
plug it into the equation and solve successively for the coefficients. If you haven't done that before, do the exercise.

This tells us, in general, though, that \(t'\) is a function of \(x'\). So in a neighborhood of the point at infinity, \(X'\) looks like the graph of this function.
Therefore, in the chart where \(y \neq 0\), \(X'\) consists of the points given by the formula
\begin{equation}[x', 1, t' = 4x'^3 - 320a_2x'^7 + ...]\end{equation}
Let's now consider a map \(\varphi' = \varphi\) on \(X\), \(\infty \mapsto \infty\) in the Riemann sphere. \(\varphi\) sends the above point to \(\frac{x'}{t'} \in \mbb{C}\) or\(\frac{t'}{x'}\) in coordinates at \(\infty\), i.e. to \([x', t'] \in \CP{1} = S^2\). So this is a well-defined holomorphic function.

Now suppose
\begin{equation}
a_2 = 3\sum_{\substack{w \in \Gamma \\ w \neq 0}}\frac{1}{w^4},
\qquad a_4 = 5\sum_{\substack{w \in \Gamma \\ w \neq 0}}frac{1}{w^6}
\end{equation}
and let's look at the meromorphic mapping \((x, y) = (\wp(z), \wp'(z))\), i.e.
\begin{equation}z \mapsto [\wp(z), \wp'(z), 1]\end{equation}
We claim that this defines a homeomorphism from \(\mbb{C}/\Gamma\) to \(X'\).

If \(\wp'(z) \neq 0\), this is the same thing in homogeneous coordinates as
\begin{equation}\left[\frac{\wp(z)}{\wp'(z)}, 1, \frac{1}{\wp'(z)}\right]\end{equation}
Now, what does this look like? \(\wp\) begins with \(a/z^2\), whereas \(\wp'\) begins with \(b/z^3\), so \(\frac{\wp}{\wp'}\) begins with \(cz\), whereas \(\frac{1}{\wp'}\) begins with \(dz^3\). On the other hand, \(0 \mapsto \infty\), and in fact \(\Gamma \mapsto \infty\).

\TODO{February 7}

\TODO{February 10}

\TODO{February 12}

\TODO{February 14}

\TODO{February 24}

\TODO{February 26}

\TODO{February 28}

\subsection{Remark on Holomorphic Functions of Several Variables}

\subsection{Theorem of Mittey-Leffer (1877)}

\subsection{Infinite Products of Holomorphic Functions}

\section{Normal Families}

Recall that we proved that \(\mc{C}(\Omega)\) is metrizable and has \(\mc{H}(\Omega)\) as a closed subspace with the induced metric.
\begin{definition}
A metric space is \textbf{compact} if and only if every infinite sequence has an (infinite) convergent subsequence. Note that this is equivalent to topological compactness, i.e. every open cover has a finite subcover.
\end{definition}
\begin{definition}
We say that \(\mc{S} \subset \mc{C}(\Omega)\) is a \textbf{normal family} if every sequence in \(\mc{S}\) has a subsequence that convergs uniformly on compact subsets of \(\Omega\) (i.e. converges in \(\mc{C}(\Omega)\)). Thus, \(\mc{S}\) is compact if and only if it is normal and the limit functions themselves are in \(\mc{S}\).
\end{definition}
As an exercise, we can show that if \(\mc{S}\) is normal, \(\overline{\mc{S}}\) is.
\begin{lemma}
\(\mc{S}\) is normal if and only if \(\overline{\mc{S}}\) is compact.
\end{lemma}
\begin{lemma}
If \(\Omega = \bigcup_iE_i\) is a union of closed disks, then \(\mc{S} \subset \mc{C}(\Omega)\) is normal if and only if, for every \(i\), every sequence in \(\mc{S}\) converges a sequence that converges uniformly on \(E_i\).
\end{lemma}
\begin{proof}
\TODO{this}
\end{proof}

\TODO{this}

\begin{definition}
Let \(X \subset \mbb{C}\) and \(\mc{S} \subset \mc{C}(X)\). We say that \(\mc{S}\) is \textbf{equicontinuous} at \(a \in X\) if
\begin{equation}\forall \epsilon > 0, \exists \delta > 0, \forall z \in X, |z - a| < \delta \implies \forall f \in \mc{S}, |f(z) - f(a)| < \epsilon\end{equation}
\(\mc{S}\) is \textbf{equicontinuous on \(X\)} if it is equicontinous at each \(a \in X\). It is \textbf{uniformly equicontinuous on \(X\)} if
\begin{equation}\forall \epsilon > 0, \exists \delta > 0, \forall z, w \in X, |z - w| <\delta \implies \forall f \in \mc{S}, |f(z) - f(w)| < \epsilon\end{equation}
\end{definition}
A family of functions that this equicontinuous on compact sets can be shown to be uniformly equicontinuous.

\begin{theorem}[Arzela-Ascoli]
Let \(\Omega \subset \mbb{C}\) be a \textbf{domain}. Then \(\mc{S} \subset \mc{C}(\Omega)\) is normal if and only if
\begin{enumerate}

  \item \(\mc{S}\) is equicontinuous on \(\Omega\)

  \item There exists \(z_0 \in \Omega\) such that \(\{f(z_0) : f \in \mc{S}\) is a bounded subset of \(\mbb{C}\)

\end{enumerate}
\end{theorem}
For families of holomorphic functions, Arzela-Ascoli and the Cauchy inequalities give a criterion for normality:
\begin{definition}
\(\mc{S} \subset \mc{C}(\Omega)\) is \textbf{locally bounded} on \(\Omega\) if
\begin{equation}\forall z_0 \in \Omega, \exists \delta > 0, M < \infty, \forall z \in \Omega, f \in \mc{S}, |z - z_0| < \delta \implies |f(z)| \leq M\end{equation}
This is true if and only if \(\mc{S}\) is \textbf{uniformly bounded on compact subsets of \(\Omega\)}, i.e. for all \(K \subset \Omega\) compact,
\begin{equation}\exists M = M(K), \forall z \in K, \forall f \in \mc{S}, |f(z)| \leq M\end{equation}
\end{definition}
\begin{theorem}[Montel]
Let \(\mc{S} \subset \mc{H}(\Omega)\) where \(\Omega \subset \mbb{C}\) is a domain. Then the following are equivalent:
\begin{enumerate}

  \item \(\mc{S}\) is normal

  \item \(\mc{S}\) is locally bounded

  \item \(\mc{S}' = \{f' : f \in \mc{S}\}\) is locally bounded and there exists \(z_0 \in\Omega\) such that \(\{f(z_0) : f \in \mc{S}\}\) is bounded in \(\mbb{C}\).

\end{enumerate}
\end{theorem}
\begin{proof}

\TODO{this}

\end{proof}
Proof of Arzela-Ascoli:
\begin{proof}

\TODO{this}

\end{proof}
The Arzela-Ascoili theorem holds for families of continuous functions with values in a complete metric space, e.g. continuos functions with values in the Riemann sphere \(S^2\) (or the extended complex plane \(\mbb{C}^* = \mbb{C} \cup \{\infty\}\)) with the (induced) \textbf{chordal metric}
\begin{equation}d(z, w) = \frac{2|z - w|}{\sqrt{(1 + |z|)^2(1 + |w|)^2}}\end{equation}
(we note that the topology induced by \(\mbb{C}\) by the chordal metric is the usual Euclidean topology).
\begin{definition}[Normal in the chordal metric]
A family \(\mc{S}\) of continuous functions on \(\Omega\) is \textbf{normal in the chordal metric} if and only if it is equicontinuous in the chordal metric: condition (2) of the Arzela-Ascoli theorem is not needed because \(\mbb{C}^*\) or \(S^2\) is compact in this topology.
\end{definition}
We can use this definition to analyze, e.g., a family \(\mc{S}\) of meromorphic functions on \(\Omega \subset \mbb{C}\) (or \(\Omega \subset S^2\)), since these can be considered holomorphic functions with values in \(S^2\).
\begin{lemma}
Let \(\{f_n\}\) be a sequence of meromorphic functions which converges uniformly on compact subsets of the domain \(\Omega \subset \mbb{C}\) (on \(S^2\), in the chordal metric). Then the limit function is either meromorphic or identically \(\infty\).
\end{lemma}
\begin{proof}
\TODO{this}
\end{proof}
\begin{definition}[Spherical derivative]
If \(f\) is meromorphic on a domain \(\Omega \subset \mbb{C}\) (or \(S^2\)), we define the \textbf{spherical derivative of \(f\)} at \(z \in \Omega\) by
\begin{equation}f^\sharp(z) = \lim_{w \to z}\frac{d(f(z), f(w))}{|z - w|}\end{equation}
If \(z\) is not a pole, we have
\begin{equation}f^\sharp(z) = \lim_{w \to z}\frac{2|f(z) - f(w)|}{|z - w|\sqrt{(1 + |f(z)|^2)(1 + |f(w)|^2)}} = \frac{2|f'(z)|}{1 + |f(z)|^2}\end{equation}
\end{definition}
We have that
\begin{equation}\left(\frac{1}{f}\right)^\sharp = f^\sharp\end{equation}
implying that \(f^\sharp(z)\) is finite and continuous at all \(z \in \Omega\), and greater than zero at \(z\) if and only if \(f\) is one-to-one near \(z\).
\begin{theorem}[Marty's Theorem]
Let \(\mc{S}\) be a family of meromorphic functions on a domain \(\Omega\). Then \(\mc{S}\) is normal in the chordal metric if and only if
\begin{equation}\mc{S}^\sharp = \{f^\sharp : f \in \mc{S}\}\end{equation}
is bounded.
\end{theorem}
\begin{proof}
\TODO{this}
\end{proof}

\subsection{The Conformal Mapping Problem}

Let \(f\) be a holomorphism, and assume \(f'(z_0) = 0\). Then \(f^{-1}\) exists in a neighborhood of \(f(z)\), and \(f\) is \textbf{conformal} at \(z_0\) (preserves angles and their orientations). A nonconstant holomorphic mapping \(f: \Omega \to \mbb{C}\) is \textbf{open}, if it is one to one, then \(f\) is a homeomorphism onto its image \(f(\Omega)\), and \(f^{-1}\) is a holomorphism.
\begin{definition}
A \textbf{conformal} or \textbf{biholomorphic} mapping \(f: \Omega \to \Omega'\) is a holomorphic mapping with aholomorphic inverse.
\end{definition}
We are now faced with the \textbf{conformal mapping problem}:
\begin{itemize}

  \item Given domains \(\Omega, \Omega' \subset \mbb{C}\), are they biholomorphic?

  \item If so, can we find all biholomorphisms?

\end{itemize}
We note that, for \(f, g: \Omega \to \Omega'\), \(f, g\) are biholomorphisms if and only if \(g^{-1} \circ f \in \Aut\Omega\), the group of biholomorphisms of \(\Omega\) with itself. Furthermore, \(f\) induces a conjugation map
\begin{equation}\Aut\Omega \to \Aut\Omega', \quad S \mapsto f \circ S \circ f^{-1}\end{equation}
Now let's consider some examples, starting with the complex plane itself. We have that
\begin{equation}\Aut\mbb{C} = \{\text{linear transformations} \ w = az + b, \quad a \neq 0\}\end{equation}
Suppose \(w = f(z) \in \Aut\mbb{C}\). At \(\infty\), \(f\) has either an essential signularity or a pole. But we can show we don't have an essential singularity. On the other hand, what about when \(f\) is a polynomial, say of degree \(n\), then that must mean it is not one-to-one, because
\(f(z) = w\) has \(n\) distinct roots for almost every value of \(w\), except at roots of the derivative \(w = f(z), f'(z) = 0\). So \(n = 1\).

What about the Riemann sphere, \(\Aut S^2\). What should this group of biholomorphisms look like? Wec consider fractional linear transformations
\begin{equation}w = \frac{az + b}{cz + d}, \quad ad - bc \neq 0\end{equation}
These coefficient, of course, are not uniquely determined, being only determined up to a constant. The inverse of a fractional linear transformation is that given by the inverse matrix, namely
\begin{equation}\frac{dz - b}{-cz + a}\end{equation}
(uniquely determined up to a constant, so we don't have to write the \(\frac{1}{ad - bc}\)).

\begin{lemma}
Suppose \(G\) is a subgroup of \(\Aut\Omega\) such that \(G\) is transitive on \(\Omega\) and for some \(z_0\) the subgroup of automorphisms which fix this point lies inside of \(G\). Then \(G = \Aut\Omega\).
\end{lemma}
\begin{proof}
Let \(S \in \Aut\Omega\) be arbitrary. We have to show that \(S \in G\). To do so, we note that since \(G\) acts transitively, we can take \(T \in G\) such that \(T(z_0) = S(z_0)\). There exists such a \(T\) because it acts transitively. Then of course we can write
\begin{equation}S = T \circ (T^{-1} \circ S), \quad (T^{-1} \circ S)(z_0) = z_0 \implies T^{-1} \circ S \in G \implies S \in G\end{equation}
being the composition of elements of \(G\).
\end{proof}
So that's what we've shown here: the subgroup of automorphisms fixing \(\infty\) lies in this subgroup of fractional linear transformations, and the subgroup is transitive, and hence it composes the entire automorphism group \(\Aut S^2\).

Time for another example: the unit disc \(D\). So what would \(\Aut D\) look like? What would we like to show here? I guess we'd like to show that they're all fractional linear transformations, but which ones? This is sort of like what Schwarz's lemma tells you: if you have an automorphism of the disk that fixes just zero, then it should be a rotation. So in general, it's like a rotation times a factor
\begin{equation}w = e^{i\theta}\frac{z - z_0}{1 - \bar{z_0}z}\end{equation}
So how do you check that this actually is an automorphism of \(D\)? First of all, we should check that the boundary goes to the boundary. We can check this by just checking three points that are particularly nice, say \(1, i, -i\). Once we know this, we just need to check that the inside goes to the inside, since being an automorphism of \(S^2 \to S^2\), it either takes the inside to the inside biholomorphically or takes the inside to the outside. So if we add the condition \(|z_0| < 1\), we get that they take the inside to the inside.

Now how do we show that these compose \textit{all} automorphisms. It's not actually going to be by the previous lemma, rather, we will use Schwarz's lemma. Suppose \(T \in \Aut D\), and consider
\begin{equation}S(z) = e^{i\theta}\frac{z - z_0}{1 - \bar{z_0}z}, \qquad \text{where} \ z_0 = T(0), \quad \theta = \Arg T'(z_0)\end{equation}
We want to show that \(T = S\), by Schwarz's lemma. So what should we apply the lemma to? \(f = S \circ T^{-1}\) is an obvious choice (we could also try \(T \circ S^{-1}\), etc...). So what do we know? We have that:
\begin{itemize}

  \item \(f(0) = 0\)

  \item \(f(D) = D \implies [|z| < 1 \implies |f(z)| < 1]\)

\end{itemize}
So by Schwarz's lemma \(|f(z)| \leq z\) for all \(z \in D\). We can also apply Scharz's lemma to \(f^{-1}\) and we get \(|z| \leq |f(z)|\), which says that in modulus
\begin{equation}|f(z)| = |z|\end{equation}
which, again by Schwarz's lemma, says \(f\) is a rotation \(e^{i\alpha}z\). Now, we're basically done here, as choosing \(z_0 = 0\), this lies in the subgroup \(G\) and hence \(S \in G\). But we want to go further and show \(\alpha = 0\) implying \(S = T\). To do so, we merely note that
\begin{equation}S'(z_0) = T'(z_0) \implies \alpha = 0\end{equation}
So this is in fact what we really need for the Riemann mapping theorem. Let's finish this up: what's the other nice domain we should look at? The upper half plane!

So what's \(\Aut\mbb{H}^+\)? So first of all, the upper half-plane is actually biholomorphic to the unit disc \(D\), by, for example, the mapping
\begin{equation}\frac{z - i}{z + i}\end{equation}
To see this, we note that the boundary goes to the boundary, by checking the three points \(0, 1, \infty\) (which go to \(-1, -i, 1\) respectively). Of course the upper half-plane goes to the interior of the disc since \(i \mapsto 0\). Hence, the automorphism groups of \(D\) and \(\mbb{H}^+\) are the same, as we can transport elements between the two by composing with a biholomorphism as above. Geometrically, however, this is not going to tell us the form of these holomorphisms. So what should \(\Aut\mbb{H}^+\) look like in terms of holomorphisms of the Riemann sphere? Well, it should be the one with real coefficients, as it should be the subgroup of \(\Aut S^2\) taking \(\mbb{H}^+\) to itself and hence the real line to itself. So I claim the best form is
\begin{equation}w = \frac{az + b}{cz + d}, \quad a, b, c, d \in \mbb{R}\end{equation}
Since these are only determined up to a constant, for convenience we can say \(ad - bc = \pm 1\). This is the subgroup of \(\Aut S^2\) taking the real axis \(\mbb{R}\) to itself. But if it takes the upper half plane to itself, that says that
\begin{equation}\Im\left(\frac{ai + b}{ci + d}\right) = \frac{ad - bc}{c^2 + d^2} > 0 \iff ad - bc > 0 \iff ad > bc\end{equation}
So we have a subgroup \(G\) of fractional linear transformations in the form above satisfying the given conditions, i.e. with
\begin{equation}w = \frac{az + b}{cz + d}, \quad ad - bc = 1, a, b, c, d \in \mbb{R}\end{equation}
So we want to see that \(\Aut\mbb{H}^+ = G\). This time, we'll use the Lemma: \(G\) is a subgroup of \(\Aut\mbb{H}^+\), it's transitive on \(\Aut\mbb{H}^+\), as we can show that it can take \(i\) to any given element of \(\mbb{H}^+\), since
\begin{equation}i \mapsto ai + b, b > 0, ... TODO\end{equation}
So we have to show that the subgroup of \(\Aut\mbb{H}^+\) which fixes some point lies inside of this. So which point do we want to use? \(i\). How do we show this?

\(G\) is the subgroup of all fractional linear transformations which take the upper half-plane to itself. So all we have to do is show that this subgroup consists of fractional linear transformations. Why does it consist of fractional linear transformations? It's enough to show that the subgroup of \(\Aut\mbb{H}^+\) which fixes \(i\) consists of fractional linear transformations. That's because this subgroup of \(\Aut\mbb{H}^+\) which fixes \(i\) is just obtained from the group of automorphisms of \(D\) which fix zero by conjugation with \(\frac{z - i}{z + i}\). Anthing in here is a composite of three fractional linear transformations, and so is a fractional linear transformation.

\subsection{The Riemann Mapping Theorem}

So this is meant to be an exercise which should essentially be recalling things. We now attempt to prove the Riemann mapping theorem:
\begin{theorem}[Riemann mapping theorem]
Any simply connected open \(\Omega \subset \mbb{C}\) except \(\mbb{C}\) itself has a biholomorphic mapping onto the open unit disc \(D\)
\end{theorem}
We will begin by proving a series of lemmas.
\begin{lemma}
There is a biholomorphism of \(\Omega\) onto a bounded open subset of \(\mbb{C}\)
\end{lemma}
\begin{proof}
Let \(a \notin \Omega\) be a point, which exists as \(\Omega \neq \mbb{C}\). Then \(\frac{1}{z - a}\) is a nonvanishing holomorphic function on the simply connected open set \(\Omega\), and so it has a primitive, some holomorphic function \(g(z)\).

Now, a primitive of \(\frac{1}{z - a}\) is like a branch of \(\log(z - a)\), which means that
\begin{equation}z - a = e^{g(z)}\end{equation}
One thing this tells us right away is that \(g(z)\) is one to one, because \(z - a\) is one to one and if the composition of a function with something else is one to one, then that function must be one to one.

Take a point \(z_0 \in \Omega\). Since \(\Omega\) is open and \(g\) is one to one, implying it is nonconstant, there is an open disc centered at \(g(z_0)\) inside \(g(\Omega)\). Now, I claim that if you look at this disc translated by \(2\pi i\) it's outside of \(g(\Omega)\), i.e. \(E + 2\pi i \cap g(\Omega) = \varnothing\). Intuitively, this is the case because it's on a different branch of the log function. A cleaner way of saying this is that this is because \(\exp \circ g\) is one-to-one, but \(\exp\) will take two translated points to the same point, yielding a contradiction.
So then, how do we get a biholomorphism of \(\Omega\) onto a bounded open subset of \(\mbb{C}\)? Well, we have that
\begin{equation}h(z) = \frac{1}{g(z) - (g(z_0) + 2\pi i)}\end{equation}
is one to one and bounded on \(\Omega\). As \(g\) and \(h\) are biholomorphisms, there is a biholomorphism \(h \circ g\) from \(\Omega\) onto a bounded open set.
\end{proof}
So as we have a biholomorphsm from \(\Omega\) to a bounded open subset of \(\mbb{C}\), we can assume \(0 \in \Omega \subset D\) by simply making a translation and scaling appropriately. We're interested in defining a convenient normal family now, but it's going to come about in a very natural way. Let's look at the set of functions that are holomorphic in \(\Omega\) and are biholomorphisms into \(D\) taking the origin to itself, i.e. let
\begin{equation}\mc{A} = \{f \in \mc{H}(\Omega) : f \ \text{is one to one}, f(0) = 0, |f(z)| < 1\}\end{equation}
We're going to find the element of this set with the largest possible derivative at the origin, which is going to force the image to be as large as possible. We'll show that that means it must be all of \(D\).
We first show that the maximum derivative at zero is actually taken on
\begin{lemma}
\(\sup_{f \in \mc{A}}|f'(0)|\) is attained.
\end{lemma}
\begin{proof}
We note that the function from \(\mc{H}(\Omega) \to \mbb{C}\) taking \(f\) to \(|f'(0)|\) is continuous. Hence, taking
\begin{equation}\mc{B} = \{f \in \mc{A} : |f'(0) | \geq 1\} \supseteq \{\Id\} \neq \varnothing\end{equation}
it is enough to show that \(\mc{B}\) is compact. \(\mc{B}\) is a normal family, i.e. locally bounded, as it is bounded uniformly on all of \(\Omega\). That means the only thing we really have to show is that \(\mc{B}\) is closed, i.e.,
\begin{equation}f \in \mc{H}(\Omega), \exists f_n \in \mc{B}, f = \lim_{n \to \infty}f_n \implies f \in \mc{B}\end{equation}
We trivially have that
\begin{equation}f(0) = \lim_{n \to \infty}f_n(0) = \lim_{n \to \infty}0 = 0\end{equation}
\begin{equation}|f'(0)| = \lim_{n \to \infty}|f_n'(0)| \geq 1 \ \text{since} \ [1, \infty) \ \text{is closed}\end{equation}
So \(f\) is one to one because it's not constant. Then the only other thing to prove is \(|f(z)| < 1\). So what about that? We know that \(|f(z)| \leq 1\) since \((-\infty, 1]\) is closed, but \(f(z) \neq 1\) at every point \(z \in \Omega\) by the maximum modulus principle (as if it was this would imply \(f\) was constant, contradicting both the fact that it is one-to-one and that \(f(0) = 0\)).
\end{proof}
We prove the second part of the above statement, completing the theorem
\begin{lemma}
Let \(g \in \mc{A}\). Then \(g(\Omega) = D\) if and only if
\begin{equation}|g'(0)| = \sup_{f \in \mc{A}}|f'(0)|\end{equation}
\end{lemma}
\begin{proof}
\begin{itemize}

  \item ``Only if": suppose \(g \in \mc{A}\), \(g(\Omega) = D\). Let \(f \in \mc{A}\), and let \(h = f \circ g^{-1}: D \to f(\Omega) \subset D\), \(h(0) = 0\). Then \(|h'(0)| \leq 1\) by Schwarz's lemma. \(f = h \circ g\), and \(|f'(0)| \leq |g'(0)|\).

  \item ``if": suppose \(f \in \mc{A}\), \(a \in D \setminus f(\Omega)\). To find \(g \in \mc{A}\), \(|g'(0)| > |f'(0)|\), let
  \begin{equation}\varphi(z) = \frac{z - a}{1 - \bar{a}z} \implies (\varphi \circ f)(z) = \frac{f(z) - a}{1 - \bar{a}f(z)}\end{equation}
  is non-vanishing on \(\Omega\). Since \(\Omega\) is simply connected, \(\varphi \circ f\) has holomorphic square root \(F(z)\). We let, where \(\theta(w) = w^2\),
  \begin{equation}f = \varphi^{-1} \circ \theta \circ F = \varphi^{-1} \circ \theta \circ \psi^{-1} \circ \psi \circ F, \quad \psi(\eta) = \frac{\eta - F(0)}{1 - \overline{F(0)}\eta}\end{equation}
  We define \(g = \psi \circ F\), \(h = \varphi^{-1} \circ \theta \circ \Psi^{-1}\). We have \(g \in \mc{A}\), and \(h: D \to D\), \(h(0) = 0\) and \(|h'(0)| < 1\) by Schwarz's lemma, because \(h\) is not one to one and hence is not a rotation.

\end{itemize}
\end{proof}

\TODO{March 6 notes}

We're interested in a biholomorphism \(w = f(z)\) from a polygonal region enclosed by\(z_1,...,z_n\), with \(w_k = f(z_k)\) to the unit disc, where
\begin{itemize}

  \item \(0 < \alpha_k < 2\)

  \item \(-1 < \beta_k < 1, \sum\beta_k = 2\)

  \item \(\alpha_k + \beta_k = 1\),

  \item The intersection of the line between \(z_{k - 1}\) and \(z_k\) and the line between \(z_k\) and \(z_{k + 1}\) has angle \(\alpha_k\pi\) inside the polygon and \(\beta_k\pi\) outside the polygon

\end{itemize}
We want to find a formula for the inverse function \(z = F(w)\). The statement of the theorem (though last time we wrote it as an integral) is that, for some constant \(c\),
\begin{equation}F'(w) = c\prod(w - w_k)^{\beta_k}\end{equation}
We have that \(\zeta = (z - z_k)^{1/\alpha_k}\) is invertible and maps the ``angle" \(\alpha_k\) to the half-disc. Writing
\begin{equation}w = f(z_k + \zeta^{\alpha_k}) = g(\zeta), \zeta = (w - w_k)g(w) \implies F(w) = z_k + (w - w_k)^\alpha_kG_k(w)\end{equation}
\begin{equation}\implies F'(w) = (w - w_k)^{\alpha_k - 1}G_k(w)\end{equation}
So
\begin{equation}F'(w)(w - w_k)^{\beta_k}\end{equation}
is holomorphic and nonzero near \(w_h\). So
\begin{equation}H(w) = F'(w)\prod(w - w_k)^\beta_k\end{equation}
is holomorphic and nonzero in a neighborhood of the closed unit disk. To show that \(H(w)\) is constant, it is enough to show that \(\arg H(w) = \Im\log H(w)\) is constant on \(S^1\) (this is well defined as zero is not included so there is a branch of log). This works because \(H\) is a harmonic function, and therefore we can use the Mean Value Property and the Maximum Modulus Principle.

So we just have to compute the argument. Let's look at what happens at a point \(e^{i\theta}\) on the arc between \(w_{k - 1}\) and \(w_k\). We compute
\begin{equation}\frac{d}{d\theta}F(e^{i\theta}) = F'(e^{i\theta})ie^{i\theta}\end{equation}
We have that, since \(F(e^{i\theta})\) is a parametrization of a straight line,
\begin{equation}\arg\frac{d}{d\theta}F(e^{i\theta}) = 0 \implies \arg F'(e^{i\theta}) = const - (\theta + \pi/2)\end{equation}
We have that
\begin{equation}\arg(e^{i\theta} - w_k) = \theta/2 + const \implies \arg F'(e^{i\theta})\prod(e^{i\theta} - w_k)^{\beta_k} = const - \theta + (\sum\beta_k)\frac{\theta}{2} = const\end{equation}
This shows \(\arg H(w)\) is constant on the open arc from \(w_k\) to \(w_{k + 1}\) for all \(k\), but it's continuous because \(\log H(w)\) is well-defined. Therefore, \(H\) is constant on \(S^1\), completing the proof.

As a special case for this, I wanted to look at the integral formula for a mapping onto a rectangle, because we can use the reflection principle in this case to extend this map to one on the entire constant plane, giving us a doubly periodic (elliptic) function.

I don't want to spend that time going over it, because I'm concerned about how much actual class time we're going to have left this term, so one of the thing I definitely want to go finish is some of the applications to prove the big Picard theorem, leaving one important topic in the course, namely Riemann surfaces. But go read about it in Ahlfors.


\subsection{Theorems of Montel and Picard}

Picard's big theorem says that in the neighborhood of an essential singularity a holomorphic function omits at most one complex value. That is,
\begin{theorem}[Picard's Big Theorem]
If \(z_0\) is an isolated essential singularity of a holomorphic function \(f(z)\), then \(f\) takes every complex value with one possible exception in any neighborhood \(\Omega\) of \(z_0\), i.e. \(\#(\mbb{C}\setminus f(\Omega)) \leq 1\).
\end{theorem}
So, is this \textit{really} the best possible statement of this kind: that is, is it really possible that a complex function can omit one possible value? Yes: for example, \(e^{1/z} \neq 0\), so it omits value \(0\) even though it has an essential singularity at the origin.

So, there's also Picard's Little Theorem:
\begin{theorem}[Picard's Little Theorem]
A non-constant entire function \(f\) omits at most a point, i.e. \(\#(\mbb{C}\setminus f(\mbb{C})) \leq 1\)
\end{theorem}
\begin{proof}
So why does Picard's Little Theorem follow from Picard's Big Theorem (of course, Picard proved the little theorem first)? Well, either \(\infty\) is a pole or it is an essential singularity.
\begin{itemize}

  \item If \(\infty\) is a pole, by the fundamental theorem of algebra we have that \(f\) is a polynomial (since there are no poles at any finite points). So in this case, it does take \textit{every} value.

  \item If \(\infty\) is an essential singularity, then we can apply Picard's Big Theorem to a neighborhood of \(\infty\).

\end{itemize}
\end{proof}
So, we're going to prove Picard's big theorem using the theory of normal families, and in fact we're going to deduce it as well as a closely related, very strong theorem by Montel, from a strange lemma. This is going to be a necessary and sufficient condition for normality, but we'll express it as a necessary and sufficient condition for \textit{failure} of normality:
\begin{lemma}[Zalcman's Lemma]
Let \(\mc{S}\) be a family of meromorphic functions on a domain \(\Omega\). \(f\) is \underline{not} normal in the chordal metric if and only if there is a convergent sequence \(\{a_n\} \to a_\infty \in \Omega\), a convergent sequence of positive numbers \(\{\rho_n\} \to 0\) and a sequence of functions \(\{f_n\} \subset \mc{S}\) such that the sequence
\begin{equation}g_n(z) = f_n(a_n + \rho_n z)\end{equation}
converges uniformly to \(g\) in the chordal metric on compact subsets of \(\mbb{C}\) where \(g\) is nonconstant and meromorphic on all of \(\mbb{C}\). Moreover, in the case where \(\mc{S}\) is not normal, then we can choose the data above such that
\begin{equation}\forall z \in \mbb{C}, g^\sharp(z) \leq g^\sharp(0) = 1\end{equation}
\end{lemma}
So, what's strange about this lemma? Of course, it's strange because normality is about the existence of convergent sequences: non-normal should be that the stuff doesn't converge, and yet here we're saying that we can give a criterion for non-normality in terms of a convergent sequence. So, why should that be? I mean, like, what about the case where \(\mc{S}\) \textit{were} normal and we did this kind of construction: why would we get something worse? Or \textit{would} we get something worse?
That's sort of a quandry, and the point is in the case that \(\mc{S}\) is normal, i.e. every sequence in \(\mc{S}\) contains a convergent subsequence, it's not that we get something worse, it's that we get something better: we can still perform the above construction, but \(g\) would be constant!

So, what's an example? Consider \(\mc{S} = \{f_n(z) = z^n\}\). Of course, this is uniformly convergent on compact subsets of the unit disc, and actually, is also uniformly convergent in the chordal metric on compact subsets of the complement of the closed disc. But, it's not uniformly convergent on compact subsets of a \textit{bigger} open disc, e.g. \(|z| < 2\). The issue is that it's not uniformly continuous on the unit circle, as the limit would not be continuous there. So let's look at this situation, and take \(a_n = 1\), \(\rho_n = \frac{1}{n}\). That means that
\begin{equation}g(z) = \lim_{n \to \infty}g_n(z) = \lim_{n \to \infty}\left(1 + \frac{z}{n}\right)^n = e^z\end{equation}
which is obviously nonconstant and entire. We know this from a proof from first year calculus, coming from the fact that
\begin{equation}n\log(1 + z/n) \to z \iff \frac{\log(1 + z/n)}{z/n} \to 1\end{equation}
So, how do you compute the spherical derivative of \(g\)? So, it's just from the formula,
\begin{equation}g^\sharp(z) = \frac{2|g'(z)|}{1 + |g(z)|^2} = \frac{2|e^z|}{1 + |e^z|^2} \implies g^\sharp(0) = \frac{2}{2} = 1, \quad g^\sharp(z) \leq 1 \impliedby \forall t \in \reals^+, 1 + t^2 \leq 2t\end{equation}
So that's sort of a phenomenon. Now let's prove it:
\begin{proof}
Suppose \(\mc{S}\) is normal. Then, by definition, any sequence \(\{f_n\} \subset \mc{S}\) has a convergent subsequence: let's relabel \(\{f_n\}\) to denote this subsequence, in other words, we can assume that \(\{f_n\} \to f\). Let's look at any choice of data \(\{a_n\} \to a_\infty \in \omega, \{\rho_n\} \to 0\) like in the theorem. We have
\begin{equation}g_n(z) = f_n(a_n + \rho_nz)\end{equation}
By the Arzelà–Ascoli theorem, namely that normality is equivalent to equicontinuity, we have that the sequence \(\{f_n\}\) is equicontinuous, and in fact, if we want to restrict our attention to a relatively compact neighborhood of \(a_\infty\), it will tell us that \(\{f_n\}\) is uniformly equicontinuous in a neighborhood of \(a_\infty\). That means that
\begin{equation}g(z) = \lim_{n \to \infty}g_n(z) = \lim_{n \to \infty}f_n(a_n + \rho_nz) = f(a_\infty)\end{equation}
i.e. \(g\) is a constant.
\end{proof}
We now want to prove Zalcman's lemma
\begin{proof}
Let \(\mc{S}\) be a family of meromorphic functions on a domain \(\Omega\) which is \underline{not normal} in the chordal metric. We wish to find \(a_n \to a_\infty \in \Omega\), \(\rho_n \to 0\) such that
\begin{equation}g_n(z) = f_n(a_n + \rho_n z)\end{equation}
converges to a nonconstant meromorphic function \(g\) in the chordal metric on compact subsets of \(\mbb{C}\) such that
\begin{equation}g^\sharp(z) \leq g^\sharp(0) = 1\end{equation}
So, not normal means that
\begin{equation}S^\sharp = \{f^\sharp: f \in \mc{S}\}\end{equation}
is not locally bounded (as per the theorem by Marty). This means, in particular, that there is a sequence of points \(b_n \to b_\infty \in \Omega\), \(f_n \in \mc{S}\) such that \(f_n^\sharp(b_n) \to \infty\). Of course, we can assume \(b_\infty = 0\), and hence in particular that there is some disc \(\{|z| \leq r\} \subset \Omega\).

Let
\begin{equation}M_n = \max(r - |\zeta|)f_n^\sharp(\zeta)\end{equation}
This is a continous function on a compact set, and so it takes on its maximum at some point \(a_n\), at which we have that this is equal to
\begin{equation}(r - |a_n|)f_n^\sharp(a_n)\end{equation}
In particular, \(a_n \to \infty\) since \(b_n \to 0\). So the sequence we're going to construct is
\begin{equation}g_n(z) = f_n(a_n + z/f_n^\sharp(a_n))\end{equation}
As \(n\) goes to infinity, this function is defined on bigger and bigger compact sets, as
\begin{equation}\left|a_n + \frac{z}{f_n^\sharp(a_n)}\right| \leq |a_n| + \frac{|z|}{|f_n^\sharp(a_n)|} \leq |a_n| + r - |a_n|\end{equation}
That means that this is defined on \(|z| \leq M_n\). Fix \(R \geq 0\). If \(|z| \leq R \leq M_n\), then by the chain rule for spherical derivatives
\begin{equation}|g_n^\sharp(z)| \leq \frac{|f_n^\sharp(a_n + z/f_n^\sharp(a_n))|}{|f_n^\sharp(a_n)|} \leq \frac{\cancel{M_n}}{r - |a_n + z/f_n^\sharp(a_n)|}\frac{r - |a_n|}{\cancel{M_n}} \leq \frac{r - |a_n|}{r - |a_n| - |z|/f_n^\sharp(a_n)} = \frac{1}{1  - |z|/M_n} \to 1\end{equation}
as \(n \to \infty\). This tells us by Marty's theorem that \(\{g_n\}\) contains a convergent subsequence in the chordal metric. Of course, taking that convergent subsequence and re-labeliling, we can just assume it is given by \(\{g_n\}\). \(g\) is meromorphic by this ``lemma", non-constant since \(g^\sharp(0) = 1\) and the endnote shows \(g^\sharp(z) \leq 1\).
\end{proof}

We now turn our attention to Montel's theorem. We've already seen a result of Montel, that was the consequence of the Arzela-Ascoli theorem characterising normality in terms of uniformly bounded. This is a ``much souped up" version of that theorem, which says that if you have a family \(\mc{S}\) of meromorphic functions on a domain \(\Omega\) of \(\mbb{C}\) which omits three distinct values (\textit{including} \(\infty\)) in \(\mbb{C}^*\)l is normal in the chordal metric.

We'll do this on Monday (too bad there's a virus...), and we're going to see how the Big Picard Theorem follows from this. So, Big Picard's Theorem says that if \(f\) is meromorphic in a punctured disk \(0 < |z - z_0| < \rho\) and omits three distinct values in \(\mbb{C}^*\) then it extends to be meromorphic in \(|z - z_0| < \rho\).

Of course, this doesn't look like Picard's theorem as we stated it before, but they're in fact very easily seen to be equivalent.

We now prove Montel's theorem
\begin{proof}
\TODO{this}
\end{proof}

As an exercise, show that if \(\{f_n\}\) is a sequence of holomorphic functions on a domain \(\Omega \subset \mbb{C}\) wich converges uniformly in the chordal metric, then the limit is either holomorphic or identically \(\infty\). Moreover, if the limit is holomorphic, then theconvergence is uniform in the Euclidean metric on compact subsets of \(\Omega\).

We remark that the families \(\mc{S}_n\) in the proof above are not closed (as the constant functions \(0, 1, \infty\) are in the closure). For example,
\begin{equation}\left\{z \mapsto \left(\frac{1 + z}{z}\right)^n : n \in \mbb{N}\right\}\end{equation}
omits \(0, 1\) in \(D\), but converges to \(0\) in \(D\) as \(n\) goes to infinity. However, in the proof of Montel's theorem, Zalc's lemma yields limit functions that are not constant, and therefore in the family \(\mc{S}_m\) by Hurwitz.

We can now finally move on to Picard's big theorem:
\begin{theorem}[Picard's Big Theorem]
If \(f\) is meromorphic on a punctured disc \(\{0 < |z - z_0| < \delta\}\) and \(f\) omits three distinct values in \(\mbb{C}^*\), then \(f\) extends to a meromorphic function in \(|z - z_0| < \delta\).

Equivalently, a holomorphic function omits at most one complex value in a neighborhood of an essential singularity.
\end{theorem}
\begin{proof}
\TODO{this}
\end{proof}

\section{Riemann Surfaces}

\subsection{Complex Manifolds}

The remaining topic in this course is Riemann surfaces. We will begin with an introduction to complex manifolds, with Riemann surfaces being mappings between complex manifolds of dimension one. The goal is to go through some of the basic definitions, especially for those who have not seen manifolds before. So, let's begin: what is a manifold?

\begin{definition}[Topological \(n\)-manifold]
An \textbf{\(n\)-dimensional topological manifold} is a Hausdorff, paracompact, topological space locally homeomorphic to \(\reals^n\).
\label{def:topological_manifold}
\end{definition}

In other words, an \(n\)-dimensional topological manifold means a topological space which is Hausdorff and paracompact (i.e. ``not too big", meaning every open covering has a countable subcovering) to avoid pathologies which locally ``looks like an open subset of \(\reals^n\)." That is, it can be covered with open sets \(U_i\) each of which admits a homeomorphism \(\varphi_i: U_i \to V_i\) where \(V_i \subset \reals^n\) is open. We are going to be interested in complex manifolds, which is just a manfiold to which we add a complex structure. The way we do that is that in the \hyperref[def:topological_manifold]{definition}, we simply replace \(\reals^n\) by \(\mbb{C}^n\) and we assume that the ``\textbf{transition mappings}" between any two of these coordinate charts are \textit{homeomorphisms}. That is, we define

\begin{definition}[Complex Structure]
A \textbf{\(n\)-dimensional complex structure} on a manifold \(M\) is given by an open cover of \(\{U_i\}\) of \(M\) and a set of \textbf{coordinate charts} \(\varphi_i: U_i \to \mbb{C}^n\) such that each composition (\textbf{transition mapping}) \(\varphi_i \circ \varphi_j^{-1}: \varphi_j(U_j) \to \mbb{C}^n\) is holomorphic.
\end{definition}

On manifolds, we want to talk about the usual notions of analysis, like functions and mapping between spaces and so on, and we'll want to describe their properties. The way that we'll talk about properties of a function, for example, is by saying that the function has that property in any coordinate chart. So for example, if we want to say that a continuous complex-valued function \(f: M \to \mbb{C}\) on our function is holomorphic, we'll express that by saying that for every coordinate chart \(\varphi_i\) on our manifold, the mapping \(f \circ \varphi_i^{-1}: \varphi_i(U_i) \to \mbb{C}\) is holomorphic. That is,

\begin{definition}[Holomorphic function]
We say a continuous, complex-valued function \(f: M \to \mbb{C}\) is \textbf{holomorphic} if, for each coordinate chart \(\varphi_i\) on \(M\), \(f \circ \varphi_i^{-1}: \varphi(U_i) \to \mbb{C}\)
\end{definition}

We think about coordinate charts as using a system of local coordinates to the open subset \(U_i\) of the manifold. So \(\varphi_i\) is a mapping from \(U_i\) into \(\mbb{C}^n\), so we can write it in components, i.e. \(\varphi_{i1},...,\varphi_{in}\), and the image of that corresponds to a coordinate system, well, corresponds to the standard coordinates on \(\mbb{C}^n\). So saying that \(f\) is holomorphic means that when expressed by local coordinates given by a chart, it's a holomorphic function with respect to those coordinates.

\label{example:riemann_complex_structure}
A very basic example of a complex structure on a topological manifold which we saw right at the beginning of the course is the Riemann sphere \(S^2\). Recall that we gave the Riemann sphere complex structure using \hyperref[def:stereographic]{stereographic projections}, from the north pole or from the south pole. That is, letting \(N, S\) denote the north and south poles respectively, and letting \(U = S^2 \setminus \{N\}\), \(V = S^2 \setminus \{S\}\), we can define coordinate charts
\begin{equation}\varphi_U: U \to \mbb{C} = (x, y, t) \mapsto \frac{x + iy}{1 - t}, \qquad \varphi_V: V \to \mbb{C} = (x, y, t) \mapsto \frac{x - iy}{1 + t}\end{equation}
We have that
\begin{equation}\varphi_U \circ \varphi_V^{-1}: \mbb{C} \setminus \{0\} \to \mbb{C} \setminus \{0\} = z \mapsto 1/z\end{equation}
is holomorphic, and hence this gives \(S^2\) a complex structure. Note that this is defined not everywhere, but rather on the overlap of the regions of \(\mbb{C}\) corresponding to each chart, namely \(\mbb{C} \setminus \{0\}\) (as on the north pole chart, we have \(N \mapsto \infty, S \mapsto 0\), and vice versa for the south pole chart).

Naturally, we want extend the idea of a holomorphic function to a holomorphic mapping. If we have two manifolds \(M, N\), each of which have a complex structure (i.e. each of which is covered by coordinate charts which admit mappings to open subsets of \(\mbb{C}^m\)/\(\mbb{C}^n\)) then we'll say that a continuous mapping \(f: M \to N\) is holomorphic if, when we look at it as a mapping between open subsets of \(\mbb{C}^m\) to open subsets of \(\mbb{C}^n\) by the coordinate charts, it's holomorphic. In other words, if we go from the image of \(\varphi_j\) in \(\mbb{C}^m\) back to \(M\), and then by \(f\) to \(N\), and then by \(\psi_i\) to \(\mbb{C}^n\),
we'll get a mapping which is holomorphic for all \(i\) and \(j\). Of course these composite mappings are not necessarily defined on the entire image of \(\varphi_i\), rather, they're only defined on that part of \(U_j\) which is in the inverse image of \(V_i\) by \(f\). In other words, this makes sense as a holomorphic mapping on the set on which it's well defined. So that's the idea of the following definition:

\begin{definition}[Holomorphic mapping]
If \(M, N\) are complex manifolds, \(f: M \to N\) is called a \textbf{holomorphic mapping} if, for all coordinate charts \(\varphi_j: U_j \to \mbb{C}\) of \(M\) and \(\psi_i: V_i \to \mbb{C}\) of \(N\),
\begin{equation}\psi_i \circ f \circ \varphi_j^{-1}: \varphi_j(U_j \cap f^{-1}(V_i))\end{equation}
\end{definition}

If a holomorphic mapping has an inverse which is also holomorphic, we'll say that it's an isomorphism or a biholomorphism; in other words, we have a homeomorphism between the manifolds \(f\) such that \(f, f^{-1}\) are holomorphic. That is,

\begin{definition}[Isomorphism/Biholomorphism]
A holomorphic homeomorphism \(f: M \to N\) is called an \textbf{isomorphism} or \textbf{biholomorphism} of \(M\) onto \(N\). We say that two complex structures on some topological manifold \(M\) are \textbf{equivalent} if the identity map is an isomorphism.
\end{definition}

So this is the notion of holomorphic functions and holomorphic mappings between manifolds with complex structures. Why have I been saying complex structure rather than complex manifold? So a complex structure as we've defined it means that we have a topological manifold as we've defined it together with an open covering by complex coordinates. We don't want to consider two different coverings by complex coordinate charts to define different manifolds if they're essentially the same. If for example we've got a covering by complex coordinate charts, and we add one more chart just by taking an open subset of one of the ones that's given, we shouldn't consider that to be a different manifold. So in order to avoid that situation, let's be a little bit more careful in our definition.

We've defined the idea of a complex structure, and we want to say when it makes sense to consider two complex structures to be equivalent. One simple way to say this is to require that the identity mapping between two complex structures on the same manifold is an isomorphism. Then, we can say that a topological manifold \(M\) is a topological manifold \(M\) together with an \textit{equivalence class} of complex structures, and that way we avoid the issue of whether we should consider two different complex structures as defining the same manifold or not. That is,

\begin{definition}[Complex Manifold]
A \textbf{\(n\)-dimensional complex manifold} is a topological manifold \(M\) together with an equivalence class of \(n\)-dimensional complex structures where two complex structures are considered equivalent if the identity mapping on \(M\) induces an isomorphism between them.
\end{definition}

Let's now go through some examples, all of which are things which in some sense we've already looked at in the course:
\begin{enumerate}

  \item Of course, open subsets of \(\mbb{C}\) are complex manifolds in a trivial way, being covered in one coordinate chart given by the identity mapping. Likewise, if we're given any complex manifold and take an open subset of that, we can consider the subset a complex manifold with the induced complex structure.

  \item The Riemann sphere \(S^2 = P^1(\mbb{C})\) with the complex structure assigned \hyperref[example:riemann_complex_structure]{above}. The Riemann surface of course is distinguished from a topological point of view by the fact that it's compact, and in fact can be viewed as the one-point compactification of \(\mbb{C}\) \label{example:riemann_complex_manifold}.

  \item As another example, let's look at the quotient of \(\mbb{C}\) by the action of an additive subgroup. We consider the action of the integers on \(\mbb{C}\) by taking a point \(z\) to \(z + n\). The quotient \(\mbb{C}/\mbb{Z}\) means the set of equivalence classes where two points are equivalent if their difference belongs to \(\mbb{Z}\), or equivalently if there is an \(n \in \mbb{Z}\) taking one point to another.

  This is a Hausdorff space if we give it the quotient topology, that is the induced by the projection map \(\pi: \mbb{C} \to \mbb{C}/\mbb{Z}\) where a set in \(\mbb{C}/\mbb{Z}\) is open provided that it is the image of an open set \(U \subset \mbb{C}\) in the projection \(\pi\). To place a complex manifold structure on this space, we can choose \(V \subset \mbb{C}\) small enough that \(\pi|_V\) is injective (e.g. open balls with diameter \(\leq 1\)) and define, for \(U = p(V)\), local coordinates given by \(\pi^{-1}\) (which is defined in \(U\) since \(\pi\) is injective on \(V\)).
  \label{example:cmodints}

  \item More generally, we may take \(M = \mbb{C}/\Gamma\), where \(\Gamma\) is a discrete group with generators \(e_1, e_2\) as before. We define a complex structure on \(\mbb{C}/\Gamma\) in a manner analogous to \ref{example:cmodints}.

  In contrast to the \hyperref[example:cmodints]{third example}, the situation here is significant as the quotient here is compact, as we can write \(M = p(\text{closed period parallelogram})\), i.e. the image of a compact set under a continuous map. Being the image of a closed period parallelogram, we also know that topologically the manifold \(M\) is a torus \label{example:torus_complex_manifold}.

\end{enumerate}

So already this series of examples includes two significantly different examples of compact complex manifolds, because the \hyperref[example:riemann_complex_manifold]{Riemann sphere} is compact, and the \hyperref[example:torus_complex_manifold]{quotient by a discrete group} is compact, but topologically those manifolds are very different: the Riemann sphere is a sphere, while the quotient by a discrete group is a torus, which has a hole in it. So these are basic examples of complex manifolds.

In this course, we're going to be interested almost exclusively in \textit{one-dimensional} complex manifold. One dimensional, of course, means one \textit{complex} dimension, like the complex plane \(\mbb{C}\) has one complex dimension. As real manifolds, of course, they're two dimensional, but as complex manifolds they're one dimensional. We'll call such a manifold a ``complex curve" or a ``Riemann surface", but a Riemann surface in a kind of abstract sense. When we really talk about Riemann surfaces next time, we'll mean not just a complex curve, but a complex curve with a mapping to open subsets of the Riemann sphere. But for now, we define:

\begin{definition}[Complex Curve/Abstract Riemann Surface]
A \textbf{complex curve} or \textbf{abstract Riemann surface} is a 1-dimensional complex manifold \(M\).
\end{definition}

Now, all of the usual local properties of holomorphic functions that you've studied in this or previous courses extend to complex curve. For example the principle of analytic continuation (that two holomorphic maps coincide if they coincide on a set with a limit point) extends in a simple way to manifolds, as does the maximum modulus principle, and in fact all the usual properties which are purely local.

Local notions and definitions also extend to complex manifolds. For example, we can define a meromorphic function on a complex manifold \(M\) exactly as you do on an open subset of the complex plane: you can say that locally it's given by the quotient of holomorphic functions. Another, maybe better way of saying that meromorphic means, is to say that it means a holomorphic mapping from \(M\) to the Riemann sphere, that is:
\begin{definition}[Meromorphic functions]
For a complex manifold \(M\), a holomorphic mapping \(f: M \to S^2\) is called a \textbf{meromorphic function on \(M\)}
\end{definition}
Then, a point which maps to \(\infty\) on the Riemann sphere is a pole. Examples of meromorphic functions include
\begin{enumerate}

  \item Let's go back to the quotient by a discrete group, i.e. the \hyperref[example:torus_complex_manifold]{torus}. Given a projection \(\pi: \mbb{C} \to \mbb{C}/\Gamma\), where \(\Gamma\) is a group of periods, the mapping \(f \mapsto f \circ p\) gives a bijection between meromorphic functions on \(\mbb{C}/\Gamma\) and meromorphic functions on \(\mbb{C}\) with \(\Gamma\) as group of periods.

  \item The mapping \(z \mapsto e^{2\pi iz}: \mbb{C} \to \mbb{C}^*\) induces a holomorphic mapping \(\mbb{C}/\mbb{Z} \to \mbb{C}^*\) (which is in fact one-to-one and onto, and therefore a biholomorphism and in fact an isomorphism of topological spaces).

  We note in general that a bijective holomorphic mapping of complex curves is a biholomorphism, as the inverse is clearly locally holomorphic.

\end{enumerate}

\subsection{Integration of Holomorphic Differential Forms}

Holomorphic differential forms are the objects of integration in this course. We've been working a little bit with holomorphic differential forms on open subsets of the complex plane, and they're going to be particularly useful as we begin working on integration on complex manifolds. Now remember our complex manifolds are all one-dimensional, i.e. complex curves. Rather than introducing differential forms in an abstract way, we're going to work in coordinate charts. So remember a complex manifold \(M\) is covered by a countable collection of coordinate charts \(U_i\). For each \(U_i\) in the covering, we have a homeomorphism \(\varphi_i: U_i \to V_i\) to an open subset of the complex plane. The manifold being holomorphic means that the transition mappings between any two charts, defined in the overlap of two charts, are all holomorphic. We're thinking of each of these charts \(\varphi_i\) as providing a local coordinate \(z_i \in \mbb{C}\) in eahc local coordinate chart \(U_i\). Therefore, the transition mapping gives an expression for a change of local coordinates. That is, the transition mapping \(g_{ij}\) allows us to express \(z_i\) in terms of \(z_j\) on the overlap of the two charts.

Of course, by differentiating, this gives an expression for \(dz_i\) in terms of \(dz_j\), namely by the chain rule
\begin{equation}
  z_i = g_{ij}(z_j) \implies dz_i = g_{ij}'(z_j)dz_j'
\end{equation}
Therefore, by a holomorphic differential for \(\omega\) on \(M\), we simply mean that we will have a holomorphic differential form in the usual sense on each coordinate chart. So on \(U_i, U_j\) we can wreite expressions
\begin{equation}
  \omega_i = f_i(z_i)dz_i, \qquad \omega_j = f_j(z_j)dz_j
\end{equation}
The \textbf{transition condition} means that when we consider the expression on two different coordinate charts, then one can be obtained from the other simply by the substitution formula, i.e. change of variable. That is, we have on the overlap \(U_i \cap U_j\)
\begin{equation}
  \omega_j = f_i(g_{ij}(z_i))g_{ij}'(z_i)dz_i
\end{equation}
In other words, we define a holomorphic differential form by giving an expression in every coordinate chart where one can pass from one expression to another by the usual formula for change of variables using the transition maps. We're already familiar with the very important example of the Riemann sphere.

The Riemann sphere is a compactification of the complex plane obtained by adding a point at infinity. We have our usual coordinate \(z\) in the complex plane, and we can induce a coordinate at infinity by taking a reciprocal, i.e. writing \(z = \frac{1}{z'}\) where \(z'\) denotes the coordinate at infinity, with \(z' = 0\) corresponding to the point \(\infty\). What we've done many time is that if we have a holomorphic differential form like \(\omega = f(z)dz\) in the complex plane, we can change the coordinates to those at infinity just by making the substitution \(z = \frac{1}{z'}\), giving
\begin{equation}
  z = \frac{1}{z'} \implies \omega = f(z)dz = f(1/z')dz = -\frac{f(1/z')}{z'^2}dz'
\end{equation}
That's an example of how we think about a holomorphic differential form of a manifold in general. We have an expression in each coordinate chart, and we pass from an expression in one chart to an expression in another by the usual substitution formula.

Now, since when we're working on manifolds, we're really working in a local coordinate chart, that means that any of the \textit{local} things that we know about complex analysis immediately make sense in the context of manifolds. We know, for example, that in some neighborhood of any point of the manifold, \(\omega\) has a primitive, that is a holomorphic function \(g\) such that \(dg = \omega\). We know that because if we only care about the local case, that means that we're working in a coordinate chart, and that means we're just working in an open subset of \(\mbb{C}\). And we know, on \(\mbb{C}\), that locally there is always a primitive uniquely determined up to addition of a constant.

It's not always true that there's a global primitive: we know that already on \(\mbb{C}\). However, if the manifold is simply connected, then \(\omega\) has a global primitive, and we can prove this exactly the same way as we did for open subsets of \(\mbb{C}\): namely, we integrate along curves to find the primitive. In general, of course, the integral of \(\omega\) on any closed curve needn't be zero: the objects that we get by integrating over closed curves are called \textbf{periods} of the integral.

One very important local idea that we've used in complex analysis is of course the idea of a residue. Since it's local, of course, we can always define the idea of the residue of a holomorphic differential form on a manifold, by basically repeating what we've done in the complex plane:
\begin{definition}[Residue of a holomorphic differential form]
Let \(\omega\) be a holomorphic differential form in the complement of a discrete set \(E \subset M\). Consider \(a \in E\), and let \(z\) denote local coordinates at \(a\) with \(z(a) = 0\) such that, for \(\omega_1\) holomorphic at \(a\),
\begin{equation}\omega = f(z)dz = \left(\omega_1 + \left(\frac{c_1}{z} + \frac{c_2}{z^2} + ...\right)\right)dz\end{equation}
using the Laurent expansion of \(f\) at \(a\), where \(\omega_1\) is the holomorphic differential form form consisting of the sum of terms in the Laurent series with nonnegative powers. Let \(\gamma\) be a closed path in a small neighborhood of \(a\) such that the winding number of \(\gamma\) with respect to \(a\) is \(+1\). We define the \textbf{residue} of \(\omega\) at \(a\) to be
\begin{equation}\Res_a(\omega) = \frac{1}{2\pi i}\int_\gamma\omega = c_1\end{equation}
Note in particular that this implies that \(c_1\) is independent of local coordinates.
\end{definition}

One important thing to note here is that we integrated with respect to a closed path whose winding number with respect to the point was \(+1\). It's important to know that that winding number doesn't depend on the choice of local coordinate. Why? It's because the complex plane comes with a natural orientation: given complex coordinates, it makes sense to talk about moving around a circle in the counterclockwise direction. If we change from one local coordinate to another, then the direction of counterclockwise rotation remains counterclockwise, since the transition maps are biholomorphisms and hence conformal, and therefore orientation preserving. This in particular imply that complex manifolds have a natural orientation, which implies that we can orient, e.g., the boundary of subsets in a complex manifold. That's an important thing to know in order to transfer the residue theorem to complex manifolds. That is:

\begin{theorem}[Residue Theorem]
Let \(\omega\) be a holomorphic differential form in the complement of a discrete set \(E\), and let \(\Gamma\) be the \underline{oriented} boundary of a compact set \(K\) such that \(\Gamma\) contains no point in \(E\). Then
\begin{equation}\int_\Gamma\omega = 2\pi i \sum_{a \in E \cap K}\Res_a(\omega)\end{equation}
\end{theorem}

So this is just really recalling local stuff that we know about holomorphic functions on open subsets of the complex plane and transferring them to manifolds. This is really all I wanted to say about background material: now, we can get to the main topic of this section.

\subsection{Riemann Surfaces}

What a Riemann surface is going to be is, not just a complex curve, but rather a mapping from a complex curve onto another. The target of the mapping is usually going to be the complex plane, the Riemann sphere, or an open subset of one of those two things. So we begin with a complex curve \(Y\), which is usually going to be one of the aforementioned things, and then by a Riemann surface spread over \(Y\), we mean a (non-constant) holomorphic mapping from a connected complex curve \(X\) to \(Y\). That is,
\begin{definition}[Riemann Surface]
Let \(Y\) be a complex curve (e.g. \(\mbb{C}\) or \(S^2\)). A \textbf{Riemann surface spread over \(Y\)} is a non-constant holomorphic mapping \(\varphi: X \to Y\) where \(X\) is a connected complex curve.
\end{definition}

This mapping, we're going to see, is very nice at almost every point of \(X\). The points where it's not so nice are the points that we have what are called ramification points of \(\varphi\). Recall that for a holomorphic function \(\varphi\), the \textbf{order} or \textbf{multiplicity} at a point \(z\) is the degree of the lowest degree term of the Taylor expansion of \(\varphi\) around \(z\). So \(z^k\) has multiplicity \(k\). We define:
\begin{definition}[Ramification Point]
A \textbf{ramification point} of a Riemann surface \(\varphi\) is a point with multiplicity (i.e. order) greater than one. A Riemann surface with no ramification points is called \textbf{unramified}.
\end{definition}
So, if \(\varphi\) is not ramified at a given point, that means it has multiplicity one at that point and is therefore of course a local isomorphism.

Of course, there are a number of things that we know about non-constant holomorphic mappings: they are open (the image, whether surjective or not, is an open set), we know that its ramification points are isolated, and the inverse image of any point is a discrete set. On the other hand, just like the case of any holomorphic mapping from \(\mbb{C}\) to itself, \(\varphi\) is not necessarily injective even if it's unramified.

Maybe to clarify things, it's good to think in terms of an example, and starting with really kind of the most basic example in order to illustrate what we want to think about.

\begin{enumerate}


  \item \label{example:et_riemann_surface}
  Let's take \(Y = \mbb{C} \setminus \{0\}\), and \(X = \mbb{C}\). Note that \(Y\) here is connected but not simply connected. Let's consider the Riemann surface given by the mapping \(\varphi = z \mapsto e^t\).

  Before we begin our analysis, we note that in this example, \(\varphi: X \to Y\) is in fact what's called a covering space of \(Y\). We define:
  \begin{definition}[Covering Space]
  A \textbf{covering space} of a complex manifold \(Y\) is an unramified Riemann surface such that every \(b \in Y\) has a neighborhood \(V\) such that \(\varphi^{-1}(V)\) is a disjoint union of open \(U_i \subset X\), with each mapped isomorphically onto \(V\) by \(\varphi\).
  \end{definition}
  In other words, every point in the image has a neighborhood \(V\) over which \(X\) just looks like a ``stack of pancakes" each mapped biholomorphically onto \(V\) by \(\varphi\).

  In this particular example, given a point \(b \in Y\), what can we use for the open set \(V\)? Well, given \(b \in \mbb{C} \setminus \{0\}\), we can take \(V = B_{|b|}(b)\), that is, the open disc with radius \(|b|\) centered at \(b\). So that's an open disc which does not include the origin. Since the open disc doesn't include the origin, \(\log z\) can be well-defined on that disc. Hence, each branch of \(\log z\) defines an isomorphism from \(V\) to an open set \(U_i\) in \(\mbb{C}\), and these open sets are disjoint, with their union being the entire inverse image of \(V\)
  \begin{equation}
    \bigcup_iU_i = \varphi^{-1}(V), \qquad \varphi^{-1}|_{U_i}: U_i \xrightarrow{\sim} V
  \end{equation}

  Just as an aside, for those of you who have taken topology, you probably know that \textit{any} connected open set in \(\mbb{C}\), or in fact any connected complex manifold \(Y\), has a simply connected covering space. This is a standard result in topology. We won't use it, but this is in the background of a lot of things that we're going to be discussing. So in this example, \(Y\) is not simply connected, but \(X\) is. In fact, \(X\) is what's called the \textbf{universal covering space} of \(\mbb{C} \setminus \{0\}\).

  Now, we have that \(z = e^t\) is a local coordinate in a neighborhood of any point of \(X\). \(t\) is the global coordinate on \(X\), but around any point we can use \(z\) as a local coordinate, implying that a holomorphic function on \(X\) can be expressed \underline{locally} as a holomorphic function of \(z\), but not in general \underline{globally}.

  For instance, \(t\) itself is a holomorphic function on \(X\), and can be recovered locally using a branch of \(\log\). Globally, however, we cannot define a branch of log on \(\mbb{C} \setminus \{0\}\) since we must exclude a line segment.

  Now, this is a very important way of thinking about what I just said, because it's in fact the way that we're going to use Riemann surfaces. \(\log z\) can be defined on the complex plane minus the origin, but it doesn't make sense as a single-valued function. The way to think about Riemann surfaces, then, is ``can we make this multi-valued function single valued by lifting it to some other space." In particular, we're thinking of Riemann surfaces as introducing the space \(X\) for the purpose of making a given multi-valued function single valued. What that means is that, given our multi-valued function, we want to find a space \(X\) and a mapping to \(X\) with the property that if we lift our multi-valued function to \(X\) it becomes single-value.

  In the example, this is true for a value which is extremely simple and hence maybe at first glance a little bit confusing: because \(\log \circ e^t = t\), and \(t\) is a well-defined single-valued holomorphic function on \(X\). So the purpose of introducing our Riemann surface \(X\) is in order to make our multi-valued function \(\log\) single-valued by lifting it to the Riemann surface. As in the diagram

  \begin{equation}
    \begin{tikzcd}
      \mbb{C} = X \arrow[dd, "\varphi(t) = e^t"] \arrow[rd, "t"] \\
        & \mbb{C} \\
      \mbb{C}^* = Y \arrow[ru, "\log z"]
    \end{tikzcd}
  \end{equation}

  This example and this picture is something you should really try to absorb. Let's move on to a second one now, though.

  \item \label{example:riemann_sqrt} This example will have a feature which is simpler than this because of the fact that the function \(\log z\) in the first example is a rather complicated function, because it is not algebraic. In this example, however, we consider the \textit{algebraic} relation (which we will here consider as a multi-valued function) \(y = \sqrt{x}\).

  Again, locally, outside of the origin, \(\sqrt{x}\) has a holomorphic determination. It's not single-valued, but it has two holomorphic branches. Globally, however, it's multi-valued: over any point in \(\mbb{C}\setminus\{0\}\), \(\sqrt{x}\) has two possible values. So the question is: can we make this multi-valued function simple by inducing an appropriate covering. What makes this example simple is that, as we stated above, \(\sqrt{x}\) is algebraic, i.e. it satisfies the polynomial equation \(y^2 = x \iff y^2 - x = 0\).

  We can use this algebraic equation to introduce a Riemann surface: for our Riemann surface \(X\), we'll take the set of points \((x, y)\) in \(\mbb{C}^2\) satisfying the polynomial equation. Our mapping \(\varphi\) from \(X\) into \(\mbb{C}\) is then just projection \((x, y) \mapsto x\): i.e. we write
  \begin{equation}
  \begin{tikzcd}
    X = \{(x, y) : x - y^2 = 0\} \arrow[d, "{\varphi: X \to Y = (x, y) \mapsto x}"] \\ Y = \mbb{C}
  \end{tikzcd}
  \end{equation}

  So, again, what was the purpose of this? The question was: can we make our multi-valued function \(\sqrt{x}\) single valued by lifting it to the appropriate covering space. Now one question you might have is that, for example in the case of \(\log\), we almost seem to be getting rid of the structure of the function, and replacing it with the identity. However, this is not the case: rather, we are in a sense re-expressing the structure of the function in terms of the topology of the space \(X\). \(\log\), for example, is a multi-valued function with a structure that looks somewhat complicated, but how complicated is it? It's just as complicated as the topology of the covering space described. In other words, it's expressing the structure in a purely geometric way.

  Looking again at the current example, \(\sqrt{x}\) is a multi-valued function with a somewhat complicated structure, we have to go around twice to get to the same determination, but the complication is described purely in terms of the topology of the covering space. The Riemann surface \(X\) here is just a copy of \(\mbb{C}\) (since the parabola is a well-defined function), and the multi-valued function lifts to an extremely simple map, namely just the projection from \((x, y)\) to \(y\). So the entire ``complication" in the structure of the map is expressed in the topological structure of the covering space.

  We note that \(\varphi\) is a two-to-one covering at every point except the origin: that is a reflection of the fact that the origin in \(X\) is a ramification point of \(\varphi\); \(\varphi\) is not a local isomorphism over this point. That's the difference between an unramified Riemann surface, such as in our previous example of \(\log\), where perhaps the topology was more complicated because the mapping was infinite-to-one and yet the Riemann surface is unramified, whereas in this example the covering is two-to-one but there is a ramification point.

  In the unramified case, we said that we can use \(z\) as a local coordinate in a neighborhood of any point in \(X\), but in the ramified case, we can only use \(x\) as a local coordinate on the Riemann surface at points where it's not ramified, this of course just being due to the implicit function theorem.

  \item Following the theme of last time, this is an example of a Riemann surface introduced in order to make the multi-valued function \(y = (1 - x^3)^{1/3}\) single-valued. This of course can be defined at every point \(x \in \mbb{C}\), but only as a multi-valued function. We introduce a Riemann surface in order to make this surface single valued. This is an example of an algebraic function, in that it satisfies the polynomial equation \(x^3 + y^3 = 1\). We use the subset of \(\mbb{C}^2\) satisfying this equation as a Riemann surface.
  \begin{equation}
    X = \{(x, y) \in \mbb{C}^2 : x^3 + y^3 = 1\},
    \qquad Y = \mbb{C},
    \qquad \varphi: X \to Y = (x, y) \mapsto x
  \end{equation}
  As in Example \ref{example:riemann_sqrt}, this function lifts to the very simple function \((x, y) \mapsto x\) on \(X\), giving the above commutative diagram. To see that this is a Riemann surface in the sense that we defined it last time, we have to check that \(X\) is a complex curve; in other words, a manifold.

  To show that \(X\) is a manifold, we need to consider local coordinates at a point \((x_0, y_0) \in X\). So how do we do that? As usual, if we want to check that something defined by an implicit equation, a curve like \(x^3 + y^3 = 1\), is a manifold, what do we do? We use the implicit function theorem: let's consider a point \((x_0, y_0) \in X\), and we'll try to analyze the nature of \(X\) at that point.

  \begin{itemize}

    \item If \(y_0 \neq 0\), then \(x\) is a local coordinate, with the map \(x \mapsto (x, y)\), where \(y\) is a branch of \((1 - x^3)^{1/3}\) which is equal to \(y_0\) when \(x = x_0\).

    \item If \(y_0 = 0\), then \(x_0 \neq 0\), and so \(y\) is a local coordinate with the map \(y \mapsto (x, y)\).

  \end{itemize}
  Thinking about the definition of a manifold, we should ask ourselves ``why are these two different coordinates compatible in the overlap of two coordinate charts." That means, if we're looking at a point at which we can use either \(x\) or \(y\) as a local coordinate, i.e. a point where \(x_0, y_0 \neq 0\), is the transition mapping holomorphic? The answer here is yes: we need to check that \(y\) is a holomorphic function of \(x\) and vice versa. We can do this by simply choosing brnaches of \(y = (1 - x^3)^{1/3}\) and \(x = (1 - y^3)^{1/3}\), which are locally holomorphic.

  So this accomplishes what we wanted to do: we found a one-dimensional complex manifold \(X\) and a mapping over \(\mbb{C}\) such that the given multi-valued holomorphic function \(y = (1 - x^3)^{1/3}\) becomes a single valued holomorphic function on \(X\).

  Again, the point about this is, last time we talked about this for the case \(y = \sqrt{x}\), if you replace \(\sqrt{x}\) with just the projection function, we're sort of ``losing all the information about \(\sqrt{x}\)", but we're not really, we're just describing all this information in a very conceptual, geometric way. Same here. So if we want to examine the geometry of \(X\) over \(\mbb{C}\), what does it look like? Well, \(X\) has three points lying over ``most" points in \(\mbb{C}\), corresponding to the three branches of our multi-valued function. That means that \(X\) at most points over \(\mbb{C}\), just looks like a stack of three sheets.

  There are, however, three exceptional points of \(\mbb{C}\) over which there's only a single point: namely, the points given by the cube roots of unity. Because, over these points, the multi-valued function only takes the value zero. Everywhere else, \(X\) looks like a stack of three pancakes, but the three sheets come together at these points. We're going to look a little bit more at the behaviour of those points in a minute, but before doing that, let's also ask ourselves: what happens at \(\infty\)? Do the three sheets come together as a branch point at \(\infty\), or are there distinct points at \(\infty\)?

  We answer that question by extending \(X\) to a Riemann surface defined over \(\mbb{C}\) together with \(\infty\), in other words over \(S^2\). Explicitly, remember that the Riemann sphere can also be thought of as one-dimensional complex projective space, and we can compactify \(X\) to a curve in two-dimensional complex projective space with homogeneous coordinates \([x, y, z]\). So this is going back to the discussion we were having when we were talking about the Weierstrass \(\wp\)-function, and we're going to see in fact that that's very relevant here.

  Let's recall that two-dimensional complex projective space, \(\CP{2}\), is the space with homogeneous coordinates \([x, y, z]\), and we think of our original \(\mbb{C}^2\), \((x, y)\)-space, as the subset of \(\CP{2}\) where \(z = 1\). \(\CP{2}\) itself consists of \(\mbb{C}^2\) together with the ``points at infinity", the additional points we introduce, where \(z = 0\). Of course, the points where \(z = 0\) are a copy of one-dimensional complex projective space \(\CP{1}\) with homogeneous coordinates \([x, y]\). So \(\CP{2}\) is a compactification of our complex 2-dimensional space \(\mbb{C}^2\),
  in which we take \(\mbb{C}^2\) and compactify it by adding a curve \(\CP{1}\) by infinity.

  Then, the closure of our new space \(X\) given by \(x^3 + y^3 = 1\) is given by homogeneizing it's equation, because, in terms of the homogeneous coordinates, \([x, y, z]\) is like \([x/z, y/z, 1]\), so we homogeneize the equation by making that substitution, and we get a homogeneous equation
  \begin{equation}
    \frac{x^3}{z^3} + \frac{y^3}{z^3} = 1 \iff x^3 + y^3 = z^3
  \end{equation}
  This is a globally defined complex curve \(X'\) in \(\CP{2}\), and consists of our original curve \(X\) together with what we've added at infinity. It's a closed subspace of \(\CP{2}\) so it inherits the property of being a Hausdorff space (via the subspace topology) and it itself has a manifold structure.

  As an exercise (problem 4 on the current problem set), you're supposed to complete this discussion to include the points at infinity.

  So \(X\) itself is a subspace of \(X'\), and in fact we can map \(X\) into \(X'\) via
  \begin{equation}
    (x, y) \mapsto [x, y, 1]
  \end{equation}
  Now, what else is there in \(X'\)? Well, there are the points satisfying this equation where \(z = 0\), i.e. points
  \begin{equation}
    \{[x, y, 0] : x^3 + y^3 = 0\}
  \end{equation}
  In this case we can fix \(y\), looking at \(\frac{x}{y}\), and we get three points at infinity, given by \([1, -1, 0]\), \([j, -1, 0]\) and \([j^2, -1, 0]\) where \(j = e^{2\pi i/3}\).

  That means that when we look at this structure of three sheets lying over \(\mbb{C}\), those sheets are actually distinct at infinity, meaning they don't come together or anything like that, i.e. there's not a branch point at infinity. Now, how do we see this?

  One way of constructing Riemann surfaces is to think of them by ``cutting and pasting." Remember, when we constructed the Riemann surface of \(y = \sqrt{x}\), that was given by \(x = y^2\). We got a Riemann surface that had two distinct sheets over the complex plane everywhere except at the origin. One can construct a geometric model by taking two copies of the complex plane and cutting them along a line from the branch point to \(\infty\) (e.g. the positive real axis). On one copy, we label the positive edge of the cut \(1\) and the negative edge \(2\); on the other copy, we do the opposite, and then we glue the edges with the same labels together.

  This represents the fact that, going around the origin, when we come to the branch cut, we move to the second sheet, and then go around the second sheet, until we come back again to the cut and there we're back to the first sheet. Of course, that's the way the square root behaves: there are two determinations of the square root, and in order to get back to the original determination, we have to go back to the origin twice.

  When we're looking at the Riemann surface itself, this ``cut" of course doesn't appear anywhere. What appears is the branch point: Riemann surfaces otherwise are perfectly well-defined surfaces looking like two flat pancakes lying over \(\mbb{C}\), but those two sheets come together in a more complicated way at the branch point. What happens at infinity? It could be at infinity that there are two distinct sheets, or maybe infinity is another branch point. What do you think it is in this case of \(y = \sqrt{x}\)? In this case, when we look at \(x = y^2\), how many points lie over the point at infinity: two or one?

  There's only one point: infinity is a branch point. You should really, as an exercise, homogeneize this curve as one in complex projective space, and look at what happens at infinity.

  We already did that in the case of our cubic curve \(y = (1 - x^3)^{1/3}\). Here, one way of constructing the Riemann surface is, having three branch points \(1, j, j^2\) now, we can make a cut from each of those branch points out to infinity, doing that on each of three sheets, and then glue them together, again so that we get a ``spiral pattern": when we go around each branch point from edge 1 in the first sheet to edge 1 in the second sheet, and again go around from edge 2 in the second sheet to edge 2 in the third sheet, and again from edge 3 in the third sheet back to our original point.

  There are three determinations of this multi-valued function, and again we have to go around the \textit{branch points} three times to go back to our original determination. Note in this case we can travel around the origin, situated at the midpoint of the branch points, as many times as we want without leaving a single sheet, and we can take wider paths intersecting more than one cut.

  The difference between these two examples is that in the case of \(y = \sqrt{x}\), the point at \(\infty\) was itself a branch point, whereas in this case it is not. The professor actually brought his Riemann surface construction kit to this class, namely three colored paper circles, and attempted to explicitly glue the three sheets together on camera, though using Scotch tape instead of glue.

  Now one important point: if we travel in a circle either within \textit{or} outside the unit circle, as long as we don't touch the branch points, we stay within \textit{one} sheet. While that one sheet isn't all one color (assuming the different sheets we glued together had different colors), we have three sheets joined at the branch points.

  I want to make another very important remark about this Riemann surface that we've constructed: this is a cubic curve, and we've just checked that its nonsingular. This should remind you of stuff that we talked about when we were discussing the Weierstrass \(\wp\)-function. You'll remember that I told you that \textit{every} nonsingular cubic curve can be parametrized by the Weierstrass \(\wp\)-function, and that's something that we're going to come to in the remaining lectures. But why is that? After all, this equation, \(x^3 + y^3 = z^3\), doesn't look anything like the equation that we had that was parametrized by the Weierstrass \(\wp\)-function.

  There, we had an equation like
  \begin{equation}
    y^2 = 4x^3 - 20a_2x - 28a_4
  \end{equation}
  And remember, this is like saying that \(y^2\) is equal to a cubic polynomial in \(x\) with three distinct roots. Here, our equation doesn't look like that at all. But the point is, you can put it in this form by a change of coordinates. In fact, we can make a global homogeneous change of coordinates of projective space such that this complex curve actually takes this form. How do we do that?

  Well, we can do a very simple substitution: let's change from homogeneous coordinates \([x, y, z]\) to new homogeneous coordinates \([\xi, \eta, \zeta]\) by this very simple substitution:
  \begin{equation}
    x = \xi + \eta, \qquad y = \xi - \eta, \qquad z = \zeta \implies (\xi + \eta)^3 + (\xi - \eta)^3 = 2\xi^3 + 6\xi\eta^2 = \zeta^3
  \end{equation}
  Now, remember that \(\CP{2}\) is covered by three affine coordinate charts. You can look at this curve in any of those charts by dehomogeneizing to one of the variables. Dehomogeneizing with respect to \(\xi\), i.e. dividing the equation by \(\xi^3\), we get
  \begin{equation}
    2 + 6\eta^2 = \zeta^3 \iff 6\eta^2 = \zeta^3 - 2
  \end{equation}
  So this is like \(y^2\) equals a homogeneous polynomial with three distinct roots: in fact, from here, we can just make a very simple linear change of variable, and write this as \(y^2 - x^3 - 1\). And that's precisely the kind of equation we were looking for.

  This is an example of how you can take any non-singular smooth cubic curve and by homogeneous linear transformation write it in one coordinate chart as \(y^2\) equals a cubic polynomial in \(x\) with three distinct roots: \textbf{Weierstrass normal form}.

  Now, here's a very good exercise which everybody should try to do:
  here, we have our cubic curve, and we just saw that we can make this homogeneous change of coordinate to put it in Weierstrass normal form. See if you can explicitly find a lattice such that this cubic curve is parametrized by the corresponding Weierstrass \(\wp\)-function. That's a very good exercise in putting together several themes from this course.

\end{enumerate}

\subsection{Evaluation of Integrals by Residues on Riemann Surfaces}

We now want to understand how to compute integrals using the residue calculus on a Riemann surface. We're going to work through a detailed example: the real integral
\begin{equation}
  \int_0^1\frac{dx}{(1 - x^3)^{1/3}}
\end{equation}
We're going to see how to compute this using the Riemann surface we talked about last time introduced for the purpose of making the multi-valued function which is the denominator of this integral (taken as a complex function) into a single-valued function.

The Riemann surface was given by the algebraic equation \(y = (1 - x^3)^{1/3}\) in \(\mbb{C}^2\). We described this Riemann surface in detail: of course it lies over the complex plane in three sheets except for the fact that there are three branch points at the cube roots of unity: \(1, j, j^2\). We're going to compute the integral by using the residue calculus to evaluate the complex integral over a lifting of this curve to the Riemann surface.

First of all, the question is, what \textit{does} lie on the Riemann surface over this curve? The curve lifts to three distinct pieces, each isomorphic to a piece of a curve. But what are the possibilites for the global lifting of the entire curve?

Well, it could lift to one curve that must spiral around three times to get to the original point. Or maybe it could lift to three distinct curves. And in fact, that's what we're going to show in this case: that there are three distinct closed curves whose image is the curve \(\gamma\) in this picture (not included for now).

Now, actually, this is something that we understood last time, if you like, geometrically from our construction of the Riemann surface. Last time, when we constructed the Riemann surface by gluing and pasting, we saw that outside of the branch points, we really have three distinct sheets, because if we start for example on the yellow sheet and go around, we go to red, and on the next cut we go to blue, and then we go back to red. So that's a geometric way, using our model here, of seeing that our curve \(\gamma\) in this example really lifts to three distinct curves.

But let's do it analytically. How do we show analytically that there are three distinct curves on the Riemann surface lying over \(\gamma\)? What that means is that, if we follow the way the argument of this multi-valued function varies going around the curve in the complex plane, we must get back after one passage to our original determination. That is, we'll show by computing the variation of the argument of \((1 - x^3)^{1/3}\) as \(x\) goes around \(\gamma\) that we stay in the same sheet: this is the argument principle.

As \(x\) describes \(\gamma\), by the argument principle, the argument of \((1 - x^3)^{1/3}\) varies as \(2\pi\) times the following integral:
\begin{equation}
  \frac{1}{2\pi i}\int_\gamma\frac{d[(1 - x^3)^{1/3}]}{(1 - x^3)^{1/3}}
  \label{equation:to_residue_form_gammad}
\end{equation}
So, how do we compute this integral? We have to work out precisely what this is. And probably the easiest way of making that computation is to note that in the Riemann surface we have \(\frac{dy}{y}\), and taking differentials we have
\begin{equation}
  x^3 + y^3 = 1 \implies x^2dx + y^2dy = 0 \implies \frac{dy}{y} = -\frac{x^2}{y^3}dx
  \label{equation:differential_dx_dy}
\end{equation}
So the integral in Equation \ref{equation:to_residue_form_gammad} is equal to
\begin{equation}
  -\frac{1}{2\pi i}\int_\gamma\frac{x^2dx}{1 - x^3}
  \label{equation:to_residue_form_gamma3}
\end{equation}
Now, this is a perfectly good integral of a holomorphic function, so letting \(f(x) = \frac{x^2}{1 - x^3}\), by the residue theorem, we have that Equation \ref{equation:to_residue_form_gamma3} is equal to the sum of the residues of \(f\) inside gamma, i.e.
\begin{equation}
  -(\Res(f, 1) + \Res(f, j) + \Res(f, j^2))
  \label{equation:residues_in_gamma_riemann}
\end{equation}
We can compute the residue at those poles in the usual way. For example, if we wanted to compute the residue at \(x = 1\), then we look at our function in coordinates centered at \(x = 1\) (writing \(x = 1 + t\), and then the residue is the coefficient of \(\frac{1}{t}\) in
\begin{equation}
  \frac{(1 + t)^2}{1 - (1 + t)^3} \in -\frac{(1 + t)^2}{3t + O(t^2)} \implies \Res(f, 1) = -\frac{1}{3}
\end{equation}
In general, we find that all the residues are \(-\frac{1}{3}\), giving that Equation \ref{equation:residues_in_gamma_riemann} is equal to \(-3 \cdot -\frac{1}{3} = 1\). Therefore, the variation of the argument of this multi-valued function is precisely \(2\pi \cdot 1 = 2\pi\), and that says that we come back to the original argument, and therefore to the original branch of the multi-valued function. This tells us that there must be three distinct curves lying over \(\gamma\).

So, what we're going to do is compute this real integral
\begin{equation}
  I = \int_0^1\frac{dx}{(1 - x^3)^{1/3}}
\end{equation}
that we're interested in by integrating on the Riemann surface around \textit{one} of the lifted curves lying over \(\gamma\).

So, first of all, what does it mean to integrate this ``thing" over a curve in the Riemann surface. So of course it means that we're looking at \(\frac{dx}{(1 - x^3)^{1/3}}\) as a differential form \(\omega\) on the Riemann surface. But why is it true that this defines a differential form \(\omega\) on the \(X\)? Well, we just have to see that the definition makes sense at any point, and hence in particular we must consider the two cases which give a description of the Riemann surface using the implicit function theorem:
\begin{itemize}

  \item Near a point \((x_0, y_0)\) with \(y_0 \neq 0\), we have that \(x\) is a local coordinate, and then we just have \(\omega = \frac{dx}{y}\) since \(y = (1 - x^3)^{1/3}\). So that's a well-defined holomorphic differential form.

  \item On the other hand, near a point \((x_0, 0)\), that means that \(y\) is a local coordinate, and then \(\omega\) should be given by the relationship between \(dx\) and \(dy\) that comes from taking the differential of the relation defining \(X\) (Equation \ref{equation:differential_dx_dy}), that is, \(\omega = -\frac{ydy}{x^2}\).

  This shows us in particular that if we define our differential form \(\omega\) as above at a point where \(y\) is a local coordinate, it agrees with the original definition at points in which both \(x\) and \(y\) are local coordinates (i.e. where the original definition is valid), which is what we require for a well-defined differential form on the Riemann surface.

\end{itemize}
In other words, the original the differential form \(\frac{dx}{(1 - x^3)^{1/3}}\), which is not really a differential form at all as it only makes sense in a multi-valued way on \(\mbb{C}\), lifts to a genuine holomorphic differential form by introducing a Riemann surface, just like the multi-valued function lifts to a genuine holomorphic function on the Riemann surface.

Ok, so now what's the strategy. We're going to integrate \(\omega\) over one of the lifted curves. So first, we must show that that integral can be expressed in terms of the thing that we're interested in, namely \(I\). The second important question is how we will compute this integral: namely, using the residue theorem. So the residue with respect to what?  Well, what we'll do is compute it with respect to the residue at \(\infty\). Well there are three points at infinity: so are we talking about the residue with respect to all those three points? No, because the curve lifts to three distinct curves, and outside of our branch points we have three distinct sheets, and we have one point at infinity on each of those sheets. So the integral around the closed curve will be given by the residue at \textit{one} of the points at infinity. Now, which one? Let's see in a minute.

First of all, what does the integral of the differential form \(\omega\) around the closed curve on the Riemann surface have to do with this original integral \(I\)? We integrate around \(\gamma\), so starting by integrating around \(0\) to \(1\), and that of course \textit{is} the original integral. Then what's the next piece? We then go around this branch point, and then integrate backwards, i.e. from 1 to 0 instead of 0 to 1. But what's the effect of going around the branch point? Going around the branch point changes the determination of our function by \(j^2\). That is simply the argument in the residue calculation that we made before: the argument changes by \(2\pi\) going around all three branch points, and the same residue calculation shows that going around one of the branch points it must change by \(\frac{2\pi}{3}\). So, \(\frac{2\pi}{3}\) in the denominator is like \(j^2\) in the numerator.

So again, calculating the integral, here we have the integral from 0 to 1, and then by the argument principle we have the \(j^2\) times the integral from 1 to 0. Then we integrate around another curve, but we're not going around another branch point, so the third piece is obtained from the second by the formula for integration by substitution. Let me leave it to you to check that that gives us the original formula for the integral from 0 to 1. The next piece we have to go around another branch point, and again according to the argument principle again by a factor of \(j^2\). And finally, one more time. Equationally,
\begin{equation}
  \int_0^1\omega + j^2\int_1^0\omega + \int_0^1\omega + j^2\int_1^0\omega + \int_0^1\omega + j^2\int_1^0\omega = 3(1 - j^2)I
\end{equation}
with the factor \(j^2\) in the second term from the argument of residue calculation above: the argument changes by \(\frac{2\pi}{3}\) going around the point \(1\). The third term is deduced from the second by substitution, and such.

\(\omega\) has poles at \(\infty\): we calculate the residues in coordinate at infinity as follows: letting \(x = \frac{1}{u}\), we obtain
\begin{equation}
  \frac{dx}{(1 - x^3)^{1/3}}
  = -\frac{du}{u^2(1 - u^{-3})^{1/3}}
  = -\frac{du}{u(u^3 - 1)^{1/3}}
\end{equation}
Using one of the branches, we obtain
\begin{equation}
  -\frac{du}{(-1)^{1/3}u(1 - u^3)^{1/3}}
\end{equation}
giving residue \((-1)^{2/3} = 1\). The residues of the other poles at \(\infty\) are \(1, j^2\). Since \(\gamma\) is negatively oriented with respect to \(\infty\), we have
\begin{equation}
  3(1 - j^2)I = -2\pi i(\text{one of the residues above})
  \label{equation:one_of_the_residues}
\end{equation}
Substituting \(j\) into \ref{equation:one_of_the_residues}, we obtain
\begin{equation}
  3(1 - j^2)I = -2\pi ij \iff 3(j^2 - j)I = -\sqrt{3}iI = -2\pi i \iff I = \frac{2\pi}{3\sqrt{3}}
\end{equation}
This must be the correct choice of residue because \(I\) is real, and the other residues would give the preceding answer multiplied by \(j\) or \(j^2\), which are both nonreal. We didn't really care whether we chose the right residue or not: it's whichever gives us a real value, because we know that our desired integral is real.

Actually, on our current problem set, you're asked to do this same computation for the integration of \(\frac{1}{(1 - x^5)^{1/5}}\). That's a very important problem. That problem on the problem set was actually on the final exam for this course last year, and a number of students actually did it not by using the residue calculus but actually by using very clever integration by substitution. So I'm not saying that this is the only way to compute such an integral, but this is a way which if you like is conceptually meaningful because it really illustrates the point of extending the geometry of the Riemann surface.

\subsection{Riemann Surface Associated with an Elliptic Curve}

We're going to study the structure of a curve in \(\mbb{C}^2\) given by an equation of the form
\begin{equation}
  y^2 = 4x^2 - 20a_2x - 28a_4
\end{equation}
where the right hand side has three roots. So this is just an arbitrary cubic polynomial where the right hand side has two roots: I wrote the polynomial with coefficients \(a_2\) and \(a_4\) simply thinking about the way this came up earlier in the year in our discussion about the Weierstrass \(\wp\)-function. But any cubic polynomial with three distinct roots can be written in this way, because the \(x^2\) term can always be eliminated by completing the cube.

As I recalled last time, we know that such a curve can be completed to a compact curve in \(\CP{2}\) simply by homogeneizing the equation. That is, we introduced another coordinate \(t\), where here \([x, y, t]\) denoted our homogeneous coordinates in \(\CP{2}\). So we're regarding our original curve as a Riemann mapping over \(\mbb{C}\), with \(\varphi(x, y) = x\), and we're considering \(X\) to be a subset of the compactified curve simply using \((x, y)\) as the coordinates in the affine chart of \(\CP{2}\) where \((x, y)\) corresponds to homogeneous coordinates \([x, y, 1]\). As we also recalled last time, we can define a holomorphic mapping \(\varphi': X' \to S^2\), which is just \(\varphi\) on \(X\) and \(\infty\) at our one extra point at infinity, \([0, 1, 0]\). That is, we have commutative diagram
\begin{equation}
\begin{tikzcd}
  \mbb{C}^2 \arrow[hookleftarrow, r] \arrow[rd, "\pi_1"]
    & X \arrow[hookrightarrow, rrr, "{(x, y) \mapsto [x, y, 1]}"] \arrow[d, "\varphi"]
    &&& X' \arrow[hookrightarrow, r] \arrow[d, "\varphi'"]
    & \CP{2}
    \\
  & \mbb{C} \arrow[hookrightarrow, rrr] &&& S^2 = \CP{1}
\end{tikzcd}
\end{equation}
So, let's suppose for a minute that \(a_2, a_4\), the coefficients, come from a discrete subgroup \(\Gamma = \gen{e_1, e_2}\) of \(\mbb{C}\), where \(e_1, e_2\) are linearly independent, according to the formula we talked about earlier in the year, that is
\begin{equation}
a_2 = 3\sum_{\substack{\omega \in \Gamma \\ \omega \neq 0}}\frac{1}{\omega^4}, \qquad a_4 = 5\sum_{\substack{\omega \in \Gamma \\ \omega \neq 0}}\frac{1}{\omega^6}
\end{equation}
So what we showed earlier in the year is that if we introduced the meromorphic transformation given by the Weierstrass \(\wp\)-function, that is,
\begin{equation}
  x = \wp(z), \qquad y = \wp'(z)
  \label{equation:meromorphic_weierstrass}
\end{equation}
then in fact this transformation defines a biholomorphic mapping with a biholomorphic inverse from \(\mbb{C}/\Gamma\) to our compactified Riemann surface \(X'\). This was something that we proved in full earlier in the year.

So, this means in particular that the inverse defines \(z\) as a holomorphic many-valued function on \(X'\) whose branches differ by constants belonging to \(\Gamma\): that's what we proved earlier in the year. Now, equation \ref{equation:meromorphic_weierstrass} tells us that
\begin{equation}
\frac{dx}{dz} = y \iff dx = ydz \iff dz = \frac{dx}{y} = \omega
\end{equation}
where \(\omega\) is the differential form described in our last class. So this multi-valued function \(z\) satisfies \(dz = \omega\), which means that even though \(z\) is a multi-valued function, locally any of its branches give us a primitive of the holomorphic differential form \(\omega\). If you go back to our discussion earlier in the course about integrating over complex curves, you could always get a local primitive of \(\omega\) by integrating from a basepoint to a given point, i.e. writing
\begin{equation}
  z = \int\omega
\end{equation}
So this is what we know really from early on in the course. What we're interested in doing is proving the \textit{converse} of this theorem. This is a very important classical theorem, and indeed one of the milestones of late 19th century mathematics due to Abel, which is a converse of what we just said. In other words,
\begin{theorem}[Abel's Theorem]
Given \(a_2, a_4\) given only that \(P(x) = 4x^2 - 20a_2x - 28a_4\) has three distinct roots, there is a discrete subgroup \(\Gamma\) of \(\mbb{C}\) such that \(a_2, a_4\) are given by Equation \ref{equation:meromorphic_weierstrass}. Moreover, the elliptic curve
\[X' = \{y^2t = 4x^3 - 20a_2xt^2 - 28a_4\} \subset \CP{2}\]
has a parametrization given by \([\wp(z), \wp'(z), 1]\).
\end{theorem}
So, in our earlier theorem, we assumed that \(a_2, a_4\) come from a discrete subgroup \(\Gamma\), but here, we're just beginning with \textit{any} cubic curve which has three distinct cubic roots (which we've shown can be written in the form above), and we're going to show that there's a discrete subgroup so that \(a_2, a_4\) are described in terms of \(\Gamma\) by Equation \ref{equation:meromorphic_weierstrass}.

I'm going to give a sketch of the proof of this: the only reason why this is a sketch and not a complete proof is that the proof actually uses some ideas from topology, and we're not assuming that everybody in the course has the necessary topological background. So what I've done is I've isolated in two lemmas the parts of the proof which involve ingredients from topology: we'll talk about the proofs using these lemmas now, and later we'll talk about how these lemmas are proved using topology. I think you'll appreciate how these lemmas are not difficult if you're familiar with the topology, and if you're not then this is meant really as a sketch of the proof.

Actually, when we talk about these lemmas on Monday, that'll really complete the material for the course.
\begin{proof}

In our last class, we studied the elliptic curve given by \(y^2 = P(x)\) as above, and we defined a holomorphic differential form \(\omega\), where \(z\) means a local primitive of \(\omega\). It's defined locally, but globally \(\omega = dz\) defines a \textit{many-valued} function on \(X'\). The first lemma is really the crucial thing:
\begin{lemma}
The different branches of the multi-valued function \(z\) are obtained from each other by adding constants that form a discrete subgroup \(\Gamma\). Moreover, this discrete subgroup is generated by two complex numbers which are linearly independent over \(\mbb{R}\).
\end{lemma}

\end{proof}

\end{document}
